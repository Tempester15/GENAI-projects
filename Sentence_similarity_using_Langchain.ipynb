{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community sentence-transformers -q"
      ],
      "metadata": {
        "id": "QmeDcY2lL7lD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYYAYYRxMFlU",
        "outputId": "56e30f26-957d-4b67-a58d-23dbf04c8ee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
        "from pdfminer.high_level import extract_text\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "vAOMydNCmMlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/Unit IV.pdf'\n",
        "text = extract_text(file_path)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIP4ft2Kmhw9",
        "outputId": "fc30cdfc-aad4-4cb2-9be6-89ff1a34f803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial Neural Network Tutorial \n",
            "\n",
            "The term \"Artificial neural network\" refers to a biologically inspired sub-field of artificial \n",
            "intelligence  modeled  after  the  brain.  An  Artificial  neural  network  is  usually  a \n",
            "computational network based on biological neural networks that construct the structure \n",
            "of the human brain. Similar to a human brain has neurons interconnected to each other, \n",
            "artificial neural networks also have neurons that are linked to each other in various layers \n",
            "of the networks. These neurons are known as nodes. \n",
            "\n",
            "What is Artificial Neural Network? \n",
            "\n",
            "The  term  \"Artificial  Neural  Network\"  is  derived  from  Biological  neural  networks  that \n",
            "develop  the  structure  of  a  human  brain.  Similar  to  the  human  brain  that  has  neurons \n",
            "interconnected  to  one  another,  artificial  neural  networks  also  have  neurons  that  are \n",
            "interconnected to one another in various layers of the networks. These neurons are known \n",
            "as nodes. Play Video \n",
            "\n",
            "The given figure illustrates the typical diagram of Biological Neural Network. \n",
            "\n",
            "The typical Artificial Neural Network looks something like the given figure. \n",
            "\n",
            " \n",
            " \n",
            "\fDendrites from Biological Neural Network represent inputs in Artificial Neural Networks, \n",
            "cell nucleus represents Nodes, synapse represents Weights, and Axon represents Output. \n",
            "\n",
            "Relationship between Biological neural network and artificial neural network: \n",
            "\n",
            "Biological Neural Network \n",
            "\n",
            "Artificial Neural Network \n",
            "\n",
            "Dendrites \n",
            "\n",
            "Cell nucleus \n",
            "\n",
            "Synapse \n",
            "\n",
            "Axon \n",
            "\n",
            "Inputs \n",
            "\n",
            "Nodes \n",
            "\n",
            "Weights \n",
            "\n",
            "Output \n",
            "\n",
            "An Artificial Neural Network in the field of Artificial intelligence where it attempts to \n",
            "mimic the network of neurons makes up a human brain so that computers will have an \n",
            "option to understand things and make decisions in a human-like manner. The artificial \n",
            "neural  network  is  designed  by  programming  computers  to  behave  simply  like \n",
            "interconnected brain cells. \n",
            "\n",
            "There are around 1000 billion neurons in the human brain. Each neuron has an association \n",
            "point somewhere in the range of 1,000 and 100,000. In the human brain, data is stored in \n",
            "such a manner as to be distributed, and we can extract more than one piece of this data \n",
            "when necessary from our memory parallelly. We can say that the human brain is made up \n",
            "of incredibly amazing parallel processors. \n",
            "\n",
            " \n",
            "\fWe can understand the artificial neural network with an example, consider an example of \n",
            "a digital logic gate that takes an input and gives an output. \"OR\" gate, which takes two \n",
            "inputs. If one or both the inputs are \"On,\" then we get \"On\" in output. If both the inputs \n",
            "are \"Off,\" then we get \"Off\" in output. Here the output depends upon input. Our brain \n",
            "does  not  perform  the  same  task.  The  outputs  to  inputs  relationship  keep  changing \n",
            "because of the neurons in our brain, which are \"learning.\" \n",
            "\n",
            "Each node in the network has some weights assigned to it. A transfer function \n",
            "is used for calculating the weighted sum of the inputs and the bias. \n",
            "\n",
            "After the transfer function has calculated the sum, the activation function \n",
            "obtains the result. Based on the output received, the activation functions fire \n",
            "the appropriate result from the node. For example, if the output received is \n",
            "above 0.5, the activation function fires a 1 otherwise it remains 0. \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\fA Computer Like a Brain \n",
            "\n",
            "Modern neuroscientists often discuss the brain as a type of computer. Neural networks \n",
            "\n",
            "aim to do the opposite: build a computer that functions like a brain. \n",
            "\n",
            "Of course, we only have a cursory understanding of the brain's extremely complex \n",
            "\n",
            "functions, but by creating a simplified simulation of how the brain processes data, we \n",
            "\n",
            "can build a type of computer that functions very differently from a standard one. \n",
            "\n",
            "Computer processors process data serially (\"in order\"). They perform many operations \n",
            "\n",
            "on a set of data, one at a time. Parallel processing (\"processing several streams at \n",
            "\n",
            "once\") significantly speeds up the computer by using multiple processors in series. \n",
            "\n",
            "In the image below, the parallel processing example requires five different processors: \n",
            "\n",
            "An artificial neural network (so called to distinguish it from the actual neural networks in \n",
            "\n",
            "the brain) has a fundamentally different structure. It's highly interconnected. This allows \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\fit to process data very quickly, learn from that data, and update its own internal \n",
            "\n",
            "structure to improve performance.  \n",
            "\n",
            "The high degree of interconnectedness, however, has some astounding effects. For \n",
            "\n",
            "example, neural networks are very good at recognizing obscure patterns in data. \n",
            "\n",
            "The architecture of an artificial neural network: \n",
            "\n",
            "To understand the concept of the architecture of an artificial neural network, we have to \n",
            "understand what a neural network consists of. In order to define a neural network that \n",
            "consists  of  a  large  number  of  artificial  neurons,  which  are  termed  units  arranged  in  a \n",
            "sequence of layers. Lets us look at various types of layers available in an artificial neural \n",
            "network. \n",
            "\n",
            "Artificial Neural Network primarily consists of three layers: \n",
            "\n",
            "Input Layer: \n",
            "\n",
            "As  the  name  suggests,  it  accepts  inputs  in  several  different  formats  provided  by  the \n",
            "programmer. \n",
            "\n",
            "Hidden Layer: \n",
            "\n",
            " \n",
            "\fThe  hidden  layer  presents  in-between  input  and  output  layers.  It  performs  all  the \n",
            "calculations to find hidden features and patterns. \n",
            "\n",
            "Output Layer: \n",
            "\n",
            "The input goes through a series of transformations using the hidden layer, which finally \n",
            "results in output that is conveyed using this layer. \n",
            "\n",
            "In a neural network, there are multiple parameters and hyperparameters that \n",
            "affect the performance of the model. The output of ANNs is mostly dependent \n",
            "on these parameters. Some of these parameters are weights, biases, learning \n",
            "rate, batch size etc. Each node in the ANN has some weight. \n",
            "\n",
            "Each node in the network has some weights assigned to it. A transfer function \n",
            "is used for calculating the weighted sum of the inputs and the bias. \n",
            "\n",
            "After the transfer function has calculated the sum, the activation function \n",
            "obtains the result. Based on the output received, the activation functions fire \n",
            "the appropriate result from the node. For example, if the output received is \n",
            "above 0.5, the activation function fires a 1 otherwise it remains 0. \n",
            "Some of the popular activation functions used in Artificial Neural Networks \n",
            "are Sigmoid, RELU, Softmax, tanh etc. \n",
            "\n",
            " \n",
            "\fBased on the value that the node has fired, we obtain the final output. Then, \n",
            "using the error functions, we calculate the discrepancies between the \n",
            "predicted output and resulting output and adjust the weights of the neural \n",
            "network through a process known as backpropagation \n",
            "\n",
            "The artificial neural network takes input and computes the weighted sum of the inputs \n",
            "and includes a bias. This computation is represented in the form of a transfer function. \n",
            "\n",
            "It determines weighted total is passed as an input to an activation function to produce \n",
            "the  output. Activation functions  choose  whether a  node  should  fire or  not. Only those \n",
            "who  are  fired  make  it  to  the  output  layer.  There  are  distinctive  activation  functions \n",
            "available that can be applied upon the sort of task we are performing. \n",
            "\n",
            "Advantages of Artificial Neural Network (ANN) \n",
            "\n",
            "Parallel processing capability: \n",
            "\n",
            "Artificial neural  networks  have a numerical value  that can perform more than one task \n",
            "simultaneously. \n",
            "\n",
            "Storing data on the entire network: \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\fData that is used in traditional programming is stored on the whole network, not on a \n",
            "database. The disappearance of a couple of pieces of data in one place doesn't prevent \n",
            "the network from working. \n",
            "\n",
            "Capability to work with incomplete knowledge: \n",
            "\n",
            "After ANN training, the information may produce output even with inadequate data. The \n",
            "loss of performance here relies upon the significance of missing data. \n",
            "\n",
            "Having a memory distribution: \n",
            "\n",
            "For  ANN  is  to  be  able  to  adapt,  it  is  important  to  determine  the  examples  and  to \n",
            "encourage the network according to the desired output by demonstrating these examples \n",
            "to  the  network.  The  succession  of  the  network  is  directly  proportional  to  the  chosen \n",
            "instances, and if the event can't appear to the network in all its aspects, it can produce \n",
            "false output. \n",
            "\n",
            "Having fault tolerance: \n",
            "\n",
            "Extortion of one or more cells of ANN does not prohibit it from generating output, and \n",
            "this feature makes the network fault-tolerance. \n",
            "\n",
            "Disadvantages of Artificial Neural Network: \n",
            "\n",
            "Assurance of proper network structure: \n",
            "\n",
            "There is no particular guideline for determining the structure of artificial neural networks. \n",
            "The appropriate network structure is accomplished through experience, trial, and error. \n",
            "\n",
            "Unrecognized behavior of the network: \n",
            "\n",
            "It is the most significant issue of ANN. When ANN produces a testing solution, it does not \n",
            "provide insight concerning why and how. It decreases trust in the network. \n",
            "\n",
            "Hardware dependence: \n",
            "\n",
            "Artificial  neural  networks  need  processors  with  parallel  processing  power,  as  per  their \n",
            "structure. Therefore, the realization of the equipment is dependent. \n",
            "\n",
            "Difficulty of showing the issue to the network: \n",
            "\n",
            "\fANNs can work with numerical data. Problems must be converted into numerical values \n",
            "before being introduced to ANN. The presentation mechanism to be resolved here will \n",
            "directly impact the performance of the network. It relies on the user's abilities. \n",
            "\n",
            "The duration of the network is unknown: \n",
            "\n",
            "The network is reduced to a specific value of the error, and this value does not give us \n",
            "optimum results. \n",
            "\n",
            "Science artificial neural networks that have steeped into the world in the mid-20th century are \n",
            "\n",
            "exponentially developing. In the present time, we have investigated the pros of artificial neural \n",
            "\n",
            "networks  and  the  issues  encountered  in  the  course  of  their  utilization.  It  should  not  be \n",
            "\n",
            "overlooked  that  the  cons  of  ANN  networks,  which  are  a  flourishing  science  branch,  are \n",
            "\n",
            "eliminated individually, and their pros are increasing day by day. It means that artificial neural \n",
            "\n",
            "networks will turn into an irreplaceable part of our lives progressively important. \n",
            "\n",
            "How do artificial neural networks work? \n",
            "\n",
            "Artificial Neural Network can be best represented as a weighted directed graph, where \n",
            "the artificial neurons form the nodes. The association between the neurons outputs and \n",
            "neuron  inputs  can  be  viewed  as  the  directed  edges  with  weights.  The  Artificial  Neural \n",
            "Network receives the input signal from the external source in the form of a pattern and \n",
            "image  in  the  form  of  a  vector.  These  inputs  are  then  mathematically  assigned  by  the \n",
            "notations x(n) for every n number of inputs. \n",
            "\n",
            "\fAfterward, each of the input is multiplied by its corresponding weights ( these weights are \n",
            "the details utilized by the artificial neural networks to solve a specific problem ). In general \n",
            "terms,  these  weights  normally  represent  the  strength  of  the  interconnection  between \n",
            "neurons inside the artificial neural network. All the weighted inputs are summarized inside \n",
            "the computing unit. \n",
            "\n",
            "If the weighted sum is equal to zero, then bias is added to make the output non-zero or \n",
            "something else to scale up to the system's response. Bias has the same input, and weight \n",
            "equals to 1. Here the total of weighted inputs can be in the range of 0 to positive infinity. \n",
            "Here, to keep the response in the limits of the desired value, a certain maximum value is \n",
            "benchmarked, and the total of weighted inputs is passed through the activation function. \n",
            "\n",
            "The activation function refers to the set of transfer functions used to achieve the desired \n",
            "output. There is a different kind of the activation function, but primarily either linear or \n",
            "non-linear sets of functions. Some of the commonly used sets of activation functions are \n",
            "the Binary, linear, and Tan hyperbolic sigmoidal activation functions. Let us take a look at \n",
            "each of them in details: \n",
            "\n",
            "Binary: \n",
            "\n",
            " \n",
            "\fIn binary activation function, the output is either a one or a 0. Here, to accomplish this, \n",
            "there is a threshold value set up. If the net weighted input of neurons is more than 1, then \n",
            "the final output of the activation function is returned as one or else the output is returned \n",
            "as 0. \n",
            "\n",
            "Sigmoidal Hyperbolic: \n",
            "\n",
            "The Sigmoidal Hyperbola function is generally seen as an \"S\" shaped curve. Here the tan \n",
            "hyperbolic function is used to approximate output from the actual net input. The function \n",
            "is defined as: \n",
            "\n",
            "F(x) = (1/1 + exp(-????x)) \n",
            "\n",
            "Where ???? is considered the Steepness parameter. \n",
            "\n",
            "Types of Artificial Neural Networks \n",
            "\n",
            "There are two important types of Artificial Neural Networks – \n",
            "\n",
            "  FeedForward Neural Network \n",
            "  FeedBack Neural Network \n",
            "\n",
            "FeedForward Artificial Neural Networks \n",
            "\n",
            "In the feedforward ANNs, the flow of information takes place only in one \n",
            "direction. That is, the flow of information is from the input layer to the hidden \n",
            "layer and finally to the output. There are no feedback loops present in this \n",
            "neural network. These type of neural networks are mostly used in supervised \n",
            "learning for instances such as classification, image recognition etc. We use \n",
            "them in cases where the data is not sequential in nature. \n",
            "\n",
            " \n",
            "\fFeedback Artificial Neural Networks \n",
            "\n",
            "In the feedback ANNs, the feedback loops are a part of it. Such type of neural \n",
            "networks are mainly for memory retention such as in the case of recurrent \n",
            "neural networks. These types of networks are most suited for areas where the \n",
            "data is sequential or time-dependent. \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\fPerceptron in Machine Learning \n",
            "\n",
            "In  Machine  Learning  and  Artificial  Intelligence,  Perceptron  is  the most  commonly  used \n",
            "term  for  all  folks.  It  is  the  primary  step  to  learn  Machine  Learning  and  Deep  Learning \n",
            "technologies,  which  consists  of  a  set  of  weights,  input  values  or  scores,  and  a \n",
            "threshold. Perceptron is a building block of an Artificial Neural Network. Initially, in \n",
            "the mid of 19th century, Mr. Frank Rosenblatt invented the Perceptron for performing \n",
            "certain calculations to detect input data capabilities or business intelligence. Perceptron \n",
            "is  a  linear  Machine  Learning  algorithm  used  for  supervised  learning  for  various  binary \n",
            "classifiers. This algorithm enables neurons to learn elements and processes them one by \n",
            "one during preparation. In this tutorial, \"Perceptron in Machine Learning,\" we will discuss \n",
            "in-depth knowledge of Perceptron and its basic functions in brief. Let's start with the basic \n",
            "introduction of Perceptron. \n",
            "\n",
            "What is the Perceptron model in Machine Learning? \n",
            "\n",
            "Perceptron  is  Machine  Learning  algorithm  for  supervised  learning  of  various  binary \n",
            "classification tasks. Further, Perceptron is also understood as an Artificial Neuron or \n",
            "neural network unit that helps to detect certain input data computations in business \n",
            "intelligence. \n",
            "\n",
            "Perceptron model is also treated as one of the best and simplest types of Artificial Neural \n",
            "networks. However, it is a supervised learning algorithm of binary classifiers. Hence, we \n",
            "can  consider  it  as  a  single-layer  neural  network  with  four  main  parameters,  i.e.,  input \n",
            "values, weights and Bias, net sum, and an activation function. \n",
            "\n",
            "What is Binary classifier in Machine Learning? \n",
            "\n",
            "In Machine Learning, binary classifiers are defined as the function that helps in deciding \n",
            "whether  input  data  can  be  represented  as  vectors  of  numbers  and  belongs  to  some \n",
            "specific class./  \n",
            "\n",
            "Binary  classifiers  can  be  considered  as  linear  classifiers.  In  simple  words,  we  can \n",
            "understand it as a classification algorithm that can predict linear predictor function \n",
            "in terms of weight and feature vectors. \n",
            "\n",
            " \n",
            " \n",
            "\fBasic Components of Perceptron \n",
            "\n",
            "Mr. Frank Rosenblatt invented the perceptron model as a binary classifier which contains \n",
            "three main components. These are as follows: \n",
            "\n",
            "o \n",
            "\n",
            "Input Nodes or Input Layer: \n",
            "\n",
            "This is the primary component of Perceptron which accepts the initial data into the system \n",
            "for further processing. Each input node contains a real numerical value. \n",
            "\n",
            "o  Wight and Bias: \n",
            "\n",
            "Weight  parameter  represents  the  strength  of  the  connection  between  units.  This  is \n",
            "another  most  important  parameter  of  Perceptron  components.  Weight  is  directly \n",
            "proportional  to  the  strength  of  the  associated  input  neuron  in  deciding  the  output. \n",
            "Further, Bias can be considered as the line of intercept in a linear equation. \n",
            "\n",
            "o  Activation Function: \n",
            "\n",
            "These are the final and important components that help to determine whether the neuron \n",
            "will fire or not. Activation Function can be considered primarily as a step function. \n",
            "\n",
            "Types of Activation functions: \n",
            "\n",
            "o  Sign function \n",
            "\n",
            " \n",
            "\fo  Step function, and \n",
            "\n",
            "o  Sigmoid function \n",
            "\n",
            "The  data  scientist  uses  the  activation  function  to  take  a  subjective  decision  based  on \n",
            "various problem statements and forms the desired outputs. Activation function may differ \n",
            "(e.g., Sign,  Step, and Sigmoid) in  perceptron models by checking  whether the  learning \n",
            "process is slow or has vanishing or exploding gradients. \n",
            "\n",
            "How does Perceptron work? \n",
            "\n",
            "In  Machine  Learning,  Perceptron  is  considered  as  a  single-layer  neural  network  that \n",
            "consists of four main parameters named input values (Input nodes), weights and Bias, net \n",
            "sum, and an activation function. The perceptron model begins with the multiplication of \n",
            "all input values and their weights, then adds these values together to create the weighted \n",
            "sum. Then this weighted sum is applied to the activation function 'f' to obtain the desired \n",
            "output. This activation function is also known as the step function and is represented \n",
            "by 'f'. \n",
            "\n",
            " \n",
            "\fThis  step  function  or  Activation  function  plays  a  vital  role  in  ensuring  that  output  is \n",
            "mapped between required values (0,1) or (-1,1). It is important to note that the weight of \n",
            "input is indicative of the strength of a node. Similarly, an input's bias value gives the ability \n",
            "to shift the activation function curve up or down. \n",
            "\n",
            "Perceptron model works in two important steps as follows: \n",
            "\n",
            "Step-1 \n",
            "\n",
            "In the first step first, multiply all input values with corresponding weight values and then \n",
            "add them to determine the weighted sum. Mathematically, we can calculate the weighted \n",
            "sum as follows: \n",
            "\n",
            "∑wi*xi = x1*w1 + x2*w2 +…wn*xn \n",
            "\n",
            "Add  a  special  term  called  bias  'b'  to  this  weighted  sum  to  improve  the  model's \n",
            "performance. \n",
            "\n",
            "∑wi*xi + b \n",
            "\n",
            "Step-2 \n",
            "\n",
            "In the second step, an activation function is applied with the above-mentioned weighted \n",
            "sum, which gives us output either in binary form or a continuous value as follows: \n",
            "\n",
            "Y = f(∑wi*xi + b) \n",
            "\n",
            " \n",
            "\fTypes of Perceptron Models \n",
            "\n",
            "Based on the layers, Perceptron models are divided into two types. These are as follows: \n",
            "\n",
            "1.  Single-layer Perceptron Model \n",
            "\n",
            "2.  Multi-layer Perceptron model \n",
            "\n",
            "Single Layer Perceptron Model: \n",
            "\n",
            "This  is  one  of  the  easiest  Artificial  neural  networks  (ANN)  types.  A  single-layered \n",
            "perceptron model consists feed-forward network and also includes a threshold transfer \n",
            "function inside the model. The main objective of the single-layer perceptron model is to \n",
            "analyze the linearly separable objects with binary outcomes. \n",
            "\n",
            "In  a  single  layer  perceptron  model,  its  algorithms  do  not  contain  recorded  data,  so  it \n",
            "begins  with  inconstantly  allocated  input  for  weight  parameters.  Further,  it  sums  up  all \n",
            "inputs (weight). After adding all inputs, if the total sum of all inputs is more than a pre-\n",
            "determined value, the model gets activated and shows the output value as +1. \n",
            "\n",
            "If the outcome is same as pre-determined or threshold value, then the performance of \n",
            "this  model  is  stated  as  satisfied,  and  weight  demand  does  not  change.  However,  this \n",
            "model consists of a few discrepancies triggered when multiple weight inputs values are \n",
            "fed  into  the  model.  Hence,  to  find  desired  output  and  minimize  errors,  some  changes \n",
            "should be necessary for the weights input. \n",
            "\n",
            "\"Single-layer perceptron can learn only linearly separable patterns.\" \n",
            "\n",
            "Multi-Layered Perceptron Model: \n",
            "\n",
            "Like a single-layer perceptron model, a multi-layer perceptron model also has the same \n",
            "model structure but has a greater number of hidden layers. \n",
            "\n",
            "The multi-layer perceptron model is also known as the Backpropagation algorithm, which \n",
            "executes in two stages as follows: \n",
            "\n",
            "o  Forward Stage: Activation functions start from the input layer in the forward stage and \n",
            "\n",
            "terminate on the output layer. \n",
            "\n",
            "\fo  Backward Stage: In the backward stage, weight and bias values are modified as per the \n",
            "\n",
            "model's  requirement.  In  this  stage,  the  error  between  actual  output  and  demanded \n",
            "\n",
            "originated backward on the output layer and ended on the input layer. \n",
            "\n",
            "Hence,  a  multi-layered  perceptron  model  has  considered  as  multiple  artificial  neural \n",
            "networks having various layers in which activation function does not remain linear, similar \n",
            "to a single layer perceptron model. Instead of linear, activation function can be executed \n",
            "as sigmoid, TanH, ReLU, etc., for deployment. \n",
            "\n",
            "A multi-layer perceptron model has greater processing power and can process linear and \n",
            "non-linear  patterns.  Further,  it  can  also  implement  logic  gates  such  as  AND,  OR,  XOR, \n",
            "NAND, NOT, XNOR, NOR. \n",
            "\n",
            "Advantages of Multi-Layer Perceptron: \n",
            "\n",
            "o  A multi-layered perceptron model can be used to solve complex non-linear problems. \n",
            "\n",
            "o \n",
            "\n",
            "o \n",
            "\n",
            "o \n",
            "\n",
            "It works well with both small and large input data. \n",
            "\n",
            "It helps us to obtain quick predictions after the training. \n",
            "\n",
            "It helps to obtain the same accuracy ratio with large as well as small data. \n",
            "\n",
            "Disadvantages of Multi-Layer Perceptron: \n",
            "\n",
            "o \n",
            "\n",
            "o \n",
            "\n",
            "In Multi-layer perceptron, computations are difficult and time-consuming. \n",
            "\n",
            "In multi-layer Perceptron, it is difficult to predict how much the dependent variable affects \n",
            "\n",
            "each independent variable. \n",
            "\n",
            "o  The model functioning depends on the quality of the training. \n",
            "\n",
            "Perceptron Function \n",
            "\n",
            "Perceptron function ''f(x)'' can be achieved as output by multiplying the input 'x' with the \n",
            "learned weight coefficient 'w'. \n",
            "\n",
            "Mathematically, we can express it as follows: \n",
            "\n",
            "f(x)=1; if w.x+b>0 \n",
            "\n",
            "otherwise, f(x)=0 \n",
            "\n",
            "\fo \n",
            "\n",
            "o \n",
            "\n",
            "o \n",
            "\n",
            "'w' represents real-valued weights vector \n",
            "\n",
            "'b' represents the bias \n",
            "\n",
            "'x' represents a vector of input x values. \n",
            "\n",
            "Characteristics of Perceptron \n",
            "\n",
            "The perceptron model has the following characteristics. \n",
            "\n",
            "1.  Perceptron is a machine learning algorithm for supervised learning of binary classifiers. \n",
            "\n",
            "2. \n",
            "\n",
            "In Perceptron, the weight coefficient is automatically learned. \n",
            "\n",
            "3. \n",
            "\n",
            "Initially, weights are multiplied with input features, and the decision is made whether the \n",
            "\n",
            "neuron is fired or not. \n",
            "\n",
            "4.  The activation function applies a step rule to check whether the weight function is greater \n",
            "\n",
            "than zero. \n",
            "\n",
            "5.  The linear decision boundary is drawn, enabling the distinction between the two linearly \n",
            "\n",
            "separable classes +1 and -1. \n",
            "\n",
            "6. \n",
            "\n",
            "If  the  added  sum  of  all  input  values  is  more  than  the  threshold  value,  it  must  have  an \n",
            "\n",
            "output signal; otherwise, no output will be shown. \n",
            "\n",
            "Limitations of Perceptron Model \n",
            "\n",
            "A perceptron model has limitations as follows: \n",
            "\n",
            "o  The output of a perceptron can only be a binary number (0 or 1) due to the hard limit \n",
            "\n",
            "transfer function. \n",
            "\n",
            "o  Perceptron can only be used to classify the linearly separable sets of input vectors. If input \n",
            "\n",
            "vectors are non-linear, it is not easy to classify them properly. \n",
            "\n",
            "Future of Perceptron \n",
            "\n",
            "The future of the Perceptron model is much bright and significant as it helps to interpret \n",
            "data by building intuitive patterns and applying them in the future. Machine learning is a \n",
            "rapidly growing technology of Artificial Intelligence that is continuously evolving and in \n",
            "the developing phase; hence the future of perceptron technology will continue to support \n",
            "and  facilitate  analytical  behavior  in  machines  that  will,  in  turn,  add  to  the  efficiency  of \n",
            "computers. \n",
            "\n",
            "\fThe perceptron model is continuously becoming more advanced and working efficiently \n",
            "on complex problems with the help of artificial neurons. \n",
            "\n",
            " \n",
            " \n",
            "\f \n",
            " \n",
            "\fPerceptron Training Rule For Linear \n",
            "Classification \n",
            "\n",
            "A perceptron unit is used to build the ANN system. \n",
            "\n",
            "A perceptron takes a vector of real-valued inputs, calculates a linear combination of \n",
            "these inputs, then outputs a 1 if the result is greater than some threshold and -1 \n",
            "otherwise. \n",
            "\n",
            " \n",
            " \n",
            " \n",
            "\fMore precisely, given inputs x1 through xn, the output o(x1, . . . , xn) computed by the \n",
            "perceptron is \n",
            "\n",
            "where each wi is a real-valued constant, or weight, that determines the contribution of \n",
            "input xi to the perceptron output. \n",
            "\n",
            "One way to learn an acceptable weight vector is to begin with random weights, then \n",
            "iteratively apply the perceptron to each training example, modifying the perceptron \n",
            "weights whenever it misclassifies an example. \n",
            "\n",
            "This process is repeated, iterating through the training examples as many times as \n",
            "needed until the perceptron classifies all training examples correctly. \n",
            "\n",
            "Weights are modified at each step according to the perceptron training rule, which \n",
            "revises the weight wi associated with input xi according to the rule. \n",
            "\n",
            "Perceptron Training Rule \n",
            "\n",
            "Perceptron_training_rule (X, η) \n",
            "\n",
            " initialize w (wi <- an initial (small) random value) \n",
            "\n",
            " repeat \n",
            "\n",
            "     for each training instance (x, tx) ∈ X \n",
            "\n",
            "         compute the real output ox = Activation(Summation(w.x)) \n",
            "\n",
            "         if (tx ≠ ox) \n",
            "\n",
            " \n",
            " \n",
            "\f             for each wi \n",
            "\n",
            "                 wi <- wi + ∆𝑤𝑖 \n",
            "\n",
            "                 ∆𝑤𝑖 <- η (tx - ox)xi \n",
            "\n",
            "             end for \n",
            "\n",
            "         end if \n",
            "\n",
            "     end for \n",
            "\n",
            " until all the training instances in X are correctly classified \n",
            "\n",
            " return w \n",
            "\n",
            "AND GATE Perceptron Training Rule \n",
            "Machine Learning \n",
            "\n",
            "ruth Table of AND Logical GATE is, \n",
            "\n",
            "Weights w1 = 1.2, w2 = 0.6, Threshold = 1 and Learning Rate n = 0.5 are given \n",
            "\n",
            "For Training Instance 1: A=0, B=0 and Target = 0 \n",
            "\n",
            "wi.xi = 0*1.2 + 0*0.6 = 0 \n",
            "\n",
            " \n",
            "\fThis is not greater than the threshold of 1, so the output = 0, Here the target is same as \n",
            "calculated output. \n",
            "\n",
            "For Training Instance 2: A=0, B=1 and Target = 0 \n",
            "\n",
            "See also  Linear Regression Solved Example with One Independent Variable \n",
            "\n",
            "wi.xi = 0*1.2 + 1*0.6 = 0.6 \n",
            "\n",
            "This is not greater than the threshold of 1, so the output = 0. Here the target is same as \n",
            "calculated output. \n",
            "\n",
            "For Training Instance 2: A=1, B=0 and Target = 0 \n",
            "\n",
            "wi.xi = 1*1.2 + 0*0.6 = 1.2 \n",
            "\n",
            "This is greater than the threshold of 1, so the output = 1. Here the target does not \n",
            "match with the calculated output. \n",
            "\n",
            "Hence we need to update the weights. \n",
            "\n",
            "Now, \n",
            "\n",
            "After updating weights are w1 = 0.7, w2 = 0.6 Threshold = 1 and Learning Rate n = 0.5 \n",
            "\n",
            "w1 = 0.7, w2 = 0.6 Threshold = 1 and Learning Rate n = 0.5 \n",
            "\n",
            "For Training Instance 1: A=0, B=0 and Target = 0 \n",
            "\n",
            " \n",
            " \n",
            " \n",
            "\fwi.xi = 0*0.7 + 0*0.6 = 0 \n",
            "\n",
            "This is not greater than the threshold of 1, so the output = 0. Here the target is same as \n",
            "calculated output. \n",
            "\n",
            "For Training Instance 2: A=0, B=1 and Target = 0 \n",
            "\n",
            "wi.xi = 0*0.7 + 1*0.6 = 0.6 \n",
            "\n",
            "This is not greater than the threshold of 1, so the output = 0. Here the target is same as \n",
            "calculated output. \n",
            "\n",
            "For Training Instance 3: A=1, B=0 and Target = 0 \n",
            "\n",
            "wi.xi = 1*0.7 + 0*0.6 = 0.7 \n",
            "\n",
            "This is not greater than the threshold of 1, so the output = 0. Here the target is same as \n",
            "calculated output. \n",
            "\n",
            "For Training Instance 4: A=1, B=1 and Target = 1 \n",
            "\n",
            "wi.xi = 1*0.7 + 1*0.6 = 1.3 \n",
            "\n",
            "This is greater than the threshold of 1, so the output = 1. Here the target is same as \n",
            "calculated output. \n",
            "\n",
            "Hence the final weights are w1= 0.7 and w2 = 0.6, Threshold = 1 and Learning Rate n = \n",
            "0.5. \n",
            "\n",
            " \n",
            "\fOR GATE Perceptron Training Rule Machine \n",
            "Learning \n",
            "\n",
            "Truth Table of OR Logical GATE is, \n",
            "\n",
            "Weights w1 = 0.6, w2 = 0.6, Threshold = 1 and Learning Rate n = 0.5 are given \n",
            "\n",
            "For Training Instance 1: A=0, B=0 and Target = 0 \n",
            "\n",
            "wi.xi = 0*0.6 + 0*0.6 = 0 \n",
            "\n",
            "This is not greater than the threshold of 1, so the output = 0. Here the target is same as \n",
            "calculated output. \n",
            "\n",
            "For Training Instance 2: A=0, B=1 and Target = 1 \n",
            "\n",
            "See also  Naïve Bayesian Classifier in Python \n",
            "\n",
            "wi.xi = 0*0.6 + 1*0.6 = 0.6 \n",
            "\n",
            "This is not greater than the threshold of 1, so the output = 0. Here the target does not \n",
            "match with calculated output. \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\fNow, \n",
            "\n",
            "Weights w1 = 0.6, w2 = 1.1, Threshold = 1 and Learning Rate n = 0.5 are given \n",
            "\n",
            "For Training Instance 1: A=0, B=0 and Target = 0 \n",
            "\n",
            "wi.xi = 0*0.6 + 0*1.1 = 0 \n",
            "\n",
            "This is not greater than the threshold of 1, so the output = 0. Here the target is same as \n",
            "calculated output. \n",
            "\n",
            "For Training Instance 2: A=0, B=1 and Target = 1 \n",
            "\n",
            "wi.xi = 0*0.6 + 1*1.1 = 1.1 \n",
            "\n",
            "This is not greater than the threshold of 1, so the output = 0. Here the target is same as \n",
            "calculated output. \n",
            "\n",
            "For Training Instance 3: A=1, B=0 and Target = 1 \n",
            "\n",
            "wi.xi = 1*0.6 + 0*1.1 = 0.6 \n",
            "\n",
            "This is not greater than the threshold of 1, so the output = 0. Here the target does not \n",
            "match with calculated output. \n",
            "\n",
            " \n",
            "\fNow, \n",
            "\n",
            "Weights w1 = 1.1, w2 = 1.1, Threshold = 1 and Learning Rate n = 0.5 are given \n",
            "\n",
            "See also  Locally Weighted Regression Algorithm in Python \n",
            "\n",
            "For Training Instance 1: A=0, B=0 and Target = 0 \n",
            "\n",
            "wi.xi = 0*2.2 + 0*1.1 = 0 \n",
            "\n",
            "This is not greater than the threshold of 1, so the output = 0. Here the target is same as \n",
            "calculated output. \n",
            "\n",
            "For Training Instance 2: A=0, B=1 and Target = 1 \n",
            "\n",
            "wi.xi = 0*1.1 + 1*1.1 = 1.1 \n",
            "\n",
            "This is not greater than the threshold of 1, so the output = 0. Here the target is same as \n",
            "calculated output. \n",
            "\n",
            "For Training Instance 3: A=1, B=0 and Target = 1 \n",
            "\n",
            "wi.xi = 1*1.1 + 0*1.1 = 1.1 \n",
            "\n",
            "This is not greater than the threshold of 1, so the output = 0. Here the target is same as \n",
            "calculated output. \n",
            "\n",
            "For Training Instance 4: A=1, B=1 and Target = 1 \n",
            "\n",
            "wi.xi = 1*1.1 + 1*1.1 = 2.2 \n",
            "\n",
            " \n",
            " \n",
            " \n",
            "\fThis is greater than the threshold of 1, so the output = 1. Here the target is same as \n",
            "calculated output. \n",
            "\n",
            "Final wieghts w1 = 1.1, w2 = 1.1 Threshold = 1 and Learning Rate n = 0.5. \n",
            "\n",
            "Gradient Descent And Delta Rule \n",
            "\n",
            "Gradient Descent and Delta Rule \n",
            "\n",
            "A set of data points are said to be linearly separable if the data can be divided into two \n",
            "classes using a straight line. If the data is not divided into two classes using a straight \n",
            "line, such data points are said to be called non-linearly separable data. \n",
            "\n",
            "Although the perceptron rule finds a successful weight vector when the training \n",
            "examples are linearly separable, it can fail to converge if the examples are not linearly \n",
            "separable. \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\fA second training rule, called the delta rule, is designed to overcome this difficulty. \n",
            "\n",
            "If the training examples are not linearly separable, the delta rule converges toward a \n",
            "best-fit approximation to the target concept. \n",
            "\n",
            "The key idea behind the delta rule is to use gradient descent to search the hypothesis \n",
            "space of possible weight vectors to find the weights that best fit the training examples. \n",
            "\n",
            "This rule is important because gradient descent provides the basis for the \n",
            "BACKPROPAGATON algorithm, which can learn networks with many interconnected \n",
            "units. It is also important because gradient descent can serve as the basis for learning algorithms that \n",
            "must search through hypothesis spaces containing many different types of continuously parameterized \n",
            "hypotheses. \n",
            "\n",
            "Derivation of Delta Rule \n",
            "\n",
            "The delta training rule is best understood by considering the task of training an \n",
            "unthresholded perceptron; that is, a linear unit for which the output o is given by \n",
            "\n",
            " \n",
            " \n",
            " \n",
            "\fThus, a linear unit corresponds to the first stage of a perceptron, without the threshold. \n",
            "\n",
            "In order to derive a weight learning rule for linear units, let us begin by specifying a \n",
            "measure for the training error of a hypothesis (weight vector), relative to the training \n",
            "examples. \n",
            "\n",
            "Although there are many ways to define this error, one common measure is \n",
            "\n",
            "where D is the set of training examples, ‘td’ is the target output for training example ‘d’, \n",
            "and od is the output of the linear unit for training example ‘d’. \n",
            "\n",
            "How to calculate the direction of steepest descent along the error surface? \n",
            "\n",
            "The direction of steepest can be found by computing the derivative of E with respect to \n",
            "each component of the vector w. This vector derivative is called the gradient of E with \n",
            "respect to w, written as, \n",
            "\n",
            "The gradient specifies the direction of steepest increase of E, the training rule for \n",
            "gradient descent is \n",
            "\n",
            "Here η is a positive constant called the learning rate, which determines the step size in \n",
            "the gradient descent search. \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\fThe negative sign is present because we want to move the weight vector in the direction \n",
            "that decreases E. \n",
            "\n",
            "This training rule can also be written in its component form, \n",
            "\n",
            "Here, \n",
            "\n",
            "Finally, \n",
            "\n",
            " \n",
            " \n",
            "\f \n",
            " \n",
            "\f \n",
            " \n",
            "\f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ihl6dgrantRy",
        "outputId": "cb7bbddd-fb81-477b-b238-ac12c00e86b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_score_with_query(path, query):\n",
        "  #extract text from the pdf file\n",
        "  text = extract_text(path)\n",
        "\n",
        "  #tokenizing the text into the sentences\n",
        "  sentences = sent_tokenize(text)\n",
        "\n",
        "  #removing the extra characters from the sentences\n",
        "  sentences = [re.sub('[^a-zA-z]', \" \", sentences[i]) for i in range(len(sentences))]\n",
        "\n",
        "  #tokenizing the sentences into words from the sentences list\n",
        "  words = [word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "  #tagging the words from the tokenized sentences and creating a tagged document\n",
        "  tagged_data = [TaggedDocument(words=words, tags=[str(idx)]) for idx, words in enumerate(words)]\n",
        "\n",
        "  #training model Doc2Vec on tagged words\n",
        "  model = Doc2Vec(vector_size=100, window=2, min_count=1, workers=4, epochs=1000)\n",
        "  model.build_vocab(tagged_data)\n",
        "  model.train(tagged_data, total_examples=model.corpus_count,epochs=model.epochs)\n",
        "\n",
        "  #creating the vector of the query\n",
        "  inferred_vector = model.infer_vector(word_tokenize(query.lower()))\n",
        "\n",
        "  #identifying similar documents\n",
        "  similar_documents = model.dv.most_similar(\n",
        "    [inferred_vector],\n",
        "    topn=len(model.dv)\n",
        "    )\n",
        "\n",
        "  #printing the similar documents and their similarity score\n",
        "  for index, score in similar_documents:\n",
        "    print(f\"Document {index}: Similarity Score: {score}\")\n",
        "    print(f\"Document Text: {sentences[int(index)]}\")\n",
        "    print()\n",
        "\n"
      ],
      "metadata": {
        "id": "f550_vbHpMss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/Unit IV.pdf'\n",
        "query = \"Perceptron in machine learning\"\n",
        "\n",
        "similarity_score_with_query(file_path, query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaBDvr9Wrhlg",
        "outputId": "a08e1a2e-e968-41d8-de75-245ed33b1647"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 147: Similarity Score: 0.5709187984466553\n",
            "Document Text: Activation function may differ \n",
            "(e.g., Sign,  Step, and Sigmoid) in  perceptron models by checking  whether the  learning \n",
            "process is slow or has vanishing or exploding gradients.\n",
            "\n",
            "Document 183: Similarity Score: 0.4719626009464264\n",
            "Document Text: Disadvantages of Multi-Layer Perceptron: \n",
            "\n",
            "o \n",
            "\n",
            "o \n",
            "\n",
            "In Multi-layer perceptron, computations are difficult and time-consuming.\n",
            "\n",
            "Document 205: Similarity Score: 0.4648797810077667\n",
            "Document Text: Machine learning is a \n",
            "rapidly growing technology of Artificial Intelligence that is continuously evolving and in \n",
            "the developing phase; hence the future of perceptron technology will continue to support \n",
            "and  facilitate  analytical  behavior  in  machines  that  will,  in  turn,  add  to  the  efficiency  of \n",
            "computers.\n",
            "\n",
            "Document 170: Similarity Score: 0.4608803987503052\n",
            "Document Text: \"Single-layer perceptron can learn only linearly separable patterns.\"\n",
            "\n",
            "Document 188: Similarity Score: 0.452726274728775\n",
            "Document Text: Characteristics of Perceptron \n",
            "\n",
            "The perceptron model has the following characteristics.\n",
            "\n",
            "Document 51: Similarity Score: 0.44606900215148926\n",
            "Document Text: Each node in the network has some weights assigned to it.\n",
            "\n",
            "Document 24: Similarity Score: 0.43315210938453674\n",
            "Document Text: Each node in the network has some weights assigned to it.\n",
            "\n",
            "Document 175: Similarity Score: 0.43084055185317993\n",
            "Document Text: Hence,  a  multi-layered  perceptron  model  has  considered  as  multiple  artificial  neural \n",
            "networks having various layers in which activation function does not remain linear, similar \n",
            "to a single layer perceptron model.\n",
            "\n",
            "Document 110: Similarity Score: 0.42956453561782837\n",
            "Document Text: ???\n",
            "\n",
            "Document 38: Similarity Score: 0.42654699087142944\n",
            "Document Text: The high degree of interconnectedness, however, has some astounding effects.\n",
            "\n",
            "Document 190: Similarity Score: 0.42486464977264404\n",
            "Document Text: Perceptron is a machine learning algorithm for supervised learning of binary classifiers.\n",
            "\n",
            "Document 160: Similarity Score: 0.4233262538909912\n",
            "Document Text: Single-layer Perceptron Model \n",
            "\n",
            "2.\n",
            "\n",
            "Document 32: Similarity Score: 0.4215580224990845\n",
            "Document Text: Computer processors process data serially (\"in order\").\n",
            "\n",
            "Document 210: Similarity Score: 0.40132489800453186\n",
            "Document Text: .\n",
            "\n",
            "Document 2: Similarity Score: 0.3971773087978363\n",
            "Document Text: Similar to a human brain has neurons interconnected to each other, \n",
            "artificial neural networks also have neurons that are linked to each other in various layers \n",
            "of the networks.\n",
            "\n",
            "Document 39: Similarity Score: 0.39211878180503845\n",
            "Document Text: For \n",
            "\n",
            "example, neural networks are very good at recognizing obscure patterns in data.\n",
            "\n",
            "Document 211: Similarity Score: 0.3866409659385681\n",
            "Document Text: .\n",
            "\n",
            "Document 23: Similarity Score: 0.38297733664512634\n",
            "Document Text: The  outputs  to  inputs  relationship  keep  changing \n",
            "because of the neurons in our brain, which are \"learning.\"\n",
            "\n",
            "Document 98: Similarity Score: 0.3771449029445648\n",
            "Document Text: Here the total of weighted inputs can be in the range of 0 to positive infinity.\n",
            "\n",
            "Document 13: Similarity Score: 0.37685370445251465\n",
            "Document Text: There are around 1000 billion neurons in the human brain.\n",
            "\n",
            "Document 59: Similarity Score: 0.3731827735900879\n",
            "Document Text: This computation is represented in the form of a transfer function.\n",
            "\n",
            "Document 76: Similarity Score: 0.37071430683135986\n",
            "Document Text: It decreases trust in the network.\n",
            "\n",
            "Document 84: Similarity Score: 0.36897727847099304\n",
            "Document Text: Science artificial neural networks that have steeped into the world in the mid-20th century are \n",
            "\n",
            "exponentially developing.\n",
            "\n",
            "Document 134: Similarity Score: 0.36627396941185\n",
            "Document Text: What is Binary classifier in Machine Learning?\n",
            "\n",
            "Document 35: Similarity Score: 0.3640139698982239\n",
            "Document Text: In the image below, the parallel processing example requires five different processors: \n",
            "\n",
            "An artificial neural network (so called to distinguish it from the actual neural networks in \n",
            "\n",
            "the brain) has a fundamentally different structure.\n",
            "\n",
            "Document 41: Similarity Score: 0.36093640327453613\n",
            "Document Text: In order to define a neural network that \n",
            "consists  of  a  large  number  of  artificial  neurons,  which  are  termed  units  arranged  in  a \n",
            "sequence of layers.\n",
            "\n",
            "Document 212: Similarity Score: 0.35946202278137207\n",
            "Document Text: , xn) computed by the \n",
            "perceptron is \n",
            "\n",
            "where each wi is a real-valued constant, or weight, that determines the contribution of \n",
            "input xi to the perceptron output.\n",
            "\n",
            "Document 225: Similarity Score: 0.35940349102020264\n",
            "Document Text: Here the target is same as \n",
            "calculated output.\n",
            "\n",
            "Document 43: Similarity Score: 0.3579058349132538\n",
            "Document Text: Artificial Neural Network primarily consists of three layers: \n",
            "\n",
            "Input Layer: \n",
            "\n",
            "As  the  name  suggests,  it  accepts  inputs  in  several  different  formats  provided  by  the \n",
            "programmer.\n",
            "\n",
            "Document 238: Similarity Score: 0.357690691947937\n",
            "Document Text: Here the target is same as \n",
            "calculated output.\n",
            "\n",
            "Document 116: Similarity Score: 0.356862872838974\n",
            "Document Text: We use \n",
            "them in cases where the data is not sequential in nature.\n",
            "\n",
            "Document 232: Similarity Score: 0.35669130086898804\n",
            "Document Text: Here the target is same as \n",
            "calculated output.\n",
            "\n",
            "Document 177: Similarity Score: 0.3565073311328888\n",
            "Document Text: A multi-layer perceptron model has greater processing power and can process linear and \n",
            "non-linear  patterns.\n",
            "\n",
            "Document 244: Similarity Score: 0.35632115602493286\n",
            "Document Text: Here the target is same as \n",
            "calculated output.\n",
            "\n",
            "Document 6: Similarity Score: 0.3560096025466919\n",
            "Document Text: Similar  to  the  human  brain  that  has  neurons \n",
            "interconnected  to  one  another,  artificial  neural  networks  also  have  neurons  that  are \n",
            "interconnected to one another in various layers of the networks.\n",
            "\n",
            "Document 229: Similarity Score: 0.3555792570114136\n",
            "Document Text: Here the target is same as \n",
            "calculated output.\n",
            "\n",
            "Document 49: Similarity Score: 0.3515189290046692\n",
            "Document Text: Some of these parameters are weights, biases, learning \n",
            "rate, batch size etc.\n",
            "\n",
            "Document 92: Similarity Score: 0.3512699604034424\n",
            "Document Text: These  inputs  are  then  mathematically  assigned  by  the \n",
            "notations x(n) for every n number of inputs.\n",
            "\n",
            "Document 227: Similarity Score: 0.35077062249183655\n",
            "Document Text: Here the target is same as \n",
            "calculated output.\n",
            "\n",
            "Document 115: Similarity Score: 0.350091814994812\n",
            "Document Text: These type of neural networks are mostly used in supervised \n",
            "learning for instances such as classification, image recognition etc.\n",
            "\n",
            "Document 233: Similarity Score: 0.34864485263824463\n",
            "Document Text: For Training Instance 2: A=0, B=1 and Target = 1 \n",
            "\n",
            "See also  Naïve Bayesian Classifier in Python \n",
            "\n",
            "wi.xi = 0*0.6 + 1*0.6 = 0.6 \n",
            "\n",
            "This is not greater than the threshold of 1, so the output = 0.\n",
            "\n",
            "Document 248: Similarity Score: 0.3480466902256012\n",
            "Document Text: Here the target is same as \n",
            "calculated output.\n",
            "\n",
            "Document 246: Similarity Score: 0.3478291928768158\n",
            "Document Text: Here the target is same as \n",
            "calculated output.\n",
            "\n",
            "Document 137: Similarity Score: 0.3467766046524048\n",
            "Document Text: Basic Components of Perceptron \n",
            "\n",
            "Mr. Frank Rosenblatt invented the perceptron model as a binary classifier which contains \n",
            "three main components.\n",
            "\n",
            "Document 263: Similarity Score: 0.34647783637046814\n",
            "Document Text: The negative sign is present because we want to move the weight vector in the direction \n",
            "that decreases E. \n",
            "\n",
            "This training rule can also be written in its component form, \n",
            "\n",
            "Here, \n",
            "\n",
            "Finally,\n",
            "\n",
            "Document 130: Similarity Score: 0.3464621603488922\n",
            "Document Text: Further, Perceptron is also understood as an Artificial Neuron or \n",
            "neural network unit that helps to detect certain input data computations in business \n",
            "intelligence.\n",
            "\n",
            "Document 123: Similarity Score: 0.345883309841156\n",
            "Document Text: Initially, in \n",
            "the mid of 19th century, Mr. Frank Rosenblatt invented the Perceptron for performing \n",
            "certain calculations to detect input data capabilities or business intelligence.\n",
            "\n",
            "Document 257: Similarity Score: 0.3455211818218231\n",
            "Document Text: It is also important because gradient descent can serve as the basis for learning algorithms that \n",
            "must search through hypothesis spaces containing many different types of continuously parameterized \n",
            "hypotheses.\n",
            "\n",
            "Document 242: Similarity Score: 0.3445053994655609\n",
            "Document Text: Here the target is same as \n",
            "calculated output.\n",
            "\n",
            "Document 197: Similarity Score: 0.3442731201648712\n",
            "Document Text: 5.\n",
            "\n",
            "Document 236: Similarity Score: 0.34218448400497437\n",
            "Document Text: Here the target is same as \n",
            "calculated output.\n",
            "\n",
            "Document 34: Similarity Score: 0.34217536449432373\n",
            "Document Text: Parallel processing (\"processing several streams at \n",
            "\n",
            "once\") significantly speeds up the computer by using multiple processors in series.\n",
            "\n",
            "Document 218: Similarity Score: 0.34074124693870544\n",
            "Document Text: Here the target is same as \n",
            "calculated output.\n",
            "\n",
            "Document 114: Similarity Score: 0.34043577313423157\n",
            "Document Text: There are no feedback loops present in this \n",
            "neural network.\n",
            "\n",
            "Document 20: Similarity Score: 0.3402515947818756\n",
            "Document Text: If both the inputs \n",
            "are \"Off,\" then we get \"Off\" in output.\n",
            "\n",
            "Document 119: Similarity Score: 0.3399253785610199\n",
            "Document Text: These types of networks are most suited for areas where the \n",
            "data is sequential or time-dependent.\n",
            "\n",
            "Document 223: Similarity Score: 0.3384208679199219\n",
            "Document Text: Here the target is same as \n",
            "calculated output.\n",
            "\n",
            "Document 171: Similarity Score: 0.3383350372314453\n",
            "Document Text: Multi-Layered Perceptron Model: \n",
            "\n",
            "Like a single-layer perceptron model, a multi-layer perceptron model also has the same \n",
            "model structure but has a greater number of hidden layers.\n",
            "\n",
            "Document 118: Similarity Score: 0.33826789259910583\n",
            "Document Text: Such type of neural \n",
            "networks are mainly for memory retention such as in the case of recurrent \n",
            "neural networks.\n",
            "\n",
            "Document 19: Similarity Score: 0.33787643909454346\n",
            "Document Text: If one or both the inputs are \"On,\" then we get \"On\" in output.\n",
            "\n",
            "Document 215: Similarity Score: 0.3339768946170807\n",
            "Document Text: Weights are modified at each step according to the perceptron training rule, which \n",
            "revises the weight wi associated with input xi according to the rule.\n",
            "\n",
            "Document 14: Similarity Score: 0.3338082432746887\n",
            "Document Text: Each neuron has an association \n",
            "point somewhere in the range of 1,000 and 100,000.\n",
            "\n",
            "Document 7: Similarity Score: 0.3311487138271332\n",
            "Document Text: These neurons are known \n",
            "as nodes.\n",
            "\n",
            "Document 56: Similarity Score: 0.3291871249675751\n",
            "Document Text: Some of the popular activation functions used in Artificial Neural Networks \n",
            "are Sigmoid, RELU, Softmax, tanh etc.\n",
            "\n",
            "Document 208: Similarity Score: 0.32727470993995667\n",
            "Document Text: A perceptron takes a vector of real-valued inputs, calculates a linear combination of \n",
            "these inputs, then outputs a 1 if the result is greater than some threshold and -1 \n",
            "otherwise.\n",
            "\n",
            "Document 172: Similarity Score: 0.3263736963272095\n",
            "Document Text: The multi-layer perceptron model is also known as the Backpropagation algorithm, which \n",
            "executes in two stages as follows: \n",
            "\n",
            "o  Forward Stage: Activation functions start from the input layer in the forward stage and \n",
            "\n",
            "terminate on the output layer.\n",
            "\n",
            "Document 126: Similarity Score: 0.32359620928764343\n",
            "Document Text: In this tutorial, \"Perceptron in Machine Learning,\" we will discuss \n",
            "in-depth knowledge of Perceptron and its basic functions in brief.\n",
            "\n",
            "Document 42: Similarity Score: 0.3211142420768738\n",
            "Document Text: Lets us look at various types of layers available in an artificial neural \n",
            "network.\n",
            "\n",
            "Document 37: Similarity Score: 0.31887075304985046\n",
            "Document Text: This allows \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\fit to process data very quickly, learn from that data, and update its own internal \n",
            "\n",
            "structure to improve performance.\n",
            "\n",
            "Document 12: Similarity Score: 0.3171088993549347\n",
            "Document Text: The artificial \n",
            "neural  network  is  designed  by  programming  computers  to  behave  simply  like \n",
            "interconnected brain cells.\n",
            "\n",
            "Document 3: Similarity Score: 0.3156989514827728\n",
            "Document Text: These neurons are known as nodes.\n",
            "\n",
            "Document 95: Similarity Score: 0.3142620921134949\n",
            "Document Text: All the weighted inputs are summarized inside \n",
            "the computing unit.\n",
            "\n",
            "Document 162: Similarity Score: 0.31280386447906494\n",
            "Document Text: A  single-layered \n",
            "perceptron model consists feed-forward network and also includes a threshold transfer \n",
            "function inside the model.\n",
            "\n",
            "Document 44: Similarity Score: 0.31103751063346863\n",
            "Document Text: Hidden Layer: \n",
            "\n",
            " \n",
            "\fThe  hidden  layer  presents  in-between  input  and  output  layers.\n",
            "\n",
            "Document 201: Similarity Score: 0.3091190457344055\n",
            "Document Text: Limitations of Perceptron Model \n",
            "\n",
            "A perceptron model has limitations as follows: \n",
            "\n",
            "o  The output of a perceptron can only be a binary number (0 or 1) due to the hard limit \n",
            "\n",
            "transfer function.\n",
            "\n",
            "Document 88: Similarity Score: 0.30475395917892456\n",
            "Document Text: How do artificial neural networks work?\n",
            "\n",
            "Document 214: Similarity Score: 0.3047149181365967\n",
            "Document Text: This process is repeated, iterating through the training examples as many times as \n",
            "needed until the perceptron classifies all training examples correctly.\n",
            "\n",
            "Document 128: Similarity Score: 0.3046610355377197\n",
            "Document Text: What is the Perceptron model in Machine Learning?\n",
            "\n",
            "Document 144: Similarity Score: 0.3025291860103607\n",
            "Document Text: o  Activation Function: \n",
            "\n",
            "These are the final and important components that help to determine whether the neuron \n",
            "will fire or not.\n",
            "\n",
            "Document 143: Similarity Score: 0.30139094591140747\n",
            "Document Text: Further, Bias can be considered as the line of intercept in a linear equation.\n",
            "\n",
            "Document 169: Similarity Score: 0.3013122081756592\n",
            "Document Text: Hence,  to  find  desired  output  and  minimize  errors,  some  changes \n",
            "should be necessary for the weights input.\n",
            "\n",
            "Document 120: Similarity Score: 0.30079448223114014\n",
            "Document Text: Perceptron in Machine Learning \n",
            "\n",
            "In  Machine  Learning  and  Artificial  Intelligence,  Perceptron  is  the most  commonly  used \n",
            "term  for  all  folks.\n",
            "\n",
            "Document 150: Similarity Score: 0.300588995218277\n",
            "Document Text: The perceptron model begins with the multiplication of \n",
            "all input values and their weights, then adds these values together to create the weighted \n",
            "sum.\n",
            "\n",
            "Document 132: Similarity Score: 0.2985001802444458\n",
            "Document Text: However, it is a supervised learning algorithm of binary classifiers.\n",
            "\n",
            "Document 207: Similarity Score: 0.29604092240333557\n",
            "Document Text: Perceptron Training Rule For Linear \n",
            "Classification \n",
            "\n",
            "A perceptron unit is used to build the ANN system.\n",
            "\n",
            "Document 66: Similarity Score: 0.2954529821872711\n",
            "Document Text: The disappearance of a couple of pieces of data in one place doesn't prevent \n",
            "the network from working.\n",
            "\n",
            "Document 252: Similarity Score: 0.2947686016559601\n",
            "Document Text: Although the perceptron rule finds a successful weight vector when the training \n",
            "examples are linearly separable, it can fail to converge if the examples are not linearly \n",
            "separable.\n",
            "\n",
            "Document 70: Similarity Score: 0.2944658696651459\n",
            "Document Text: The  succession  of  the  network  is  directly  proportional  to  the  chosen \n",
            "instances, and if the event can't appear to the network in all its aspects, it can produce \n",
            "false output.\n",
            "\n",
            "Document 145: Similarity Score: 0.2942194640636444\n",
            "Document Text: Activation Function can be considered primarily as a step function.\n",
            "\n",
            "Document 206: Similarity Score: 0.2935468554496765\n",
            "Document Text: The perceptron model is continuously becoming more advanced and working efficiently \n",
            "on complex problems with the help of artificial neurons.\n",
            "\n",
            "Document 5: Similarity Score: 0.29313385486602783\n",
            "Document Text: The  term  \"Artificial  Neural  Network\"  is  derived  from  Biological  neural  networks  that \n",
            "develop  the  structure  of  a  human  brain.\n",
            "\n",
            "Document 237: Similarity Score: 0.2910255193710327\n",
            "Document Text: For Training Instance 2: A=0, B=1 and Target = 1 \n",
            "\n",
            "wi.xi = 0*0.6 + 1*1.1 = 1.1 \n",
            "\n",
            "This is not greater than the threshold of 1, so the output = 0.\n",
            "\n",
            "Document 0: Similarity Score: 0.2889941334724426\n",
            "Document Text: Artificial Neural Network Tutorial \n",
            "\n",
            "The term \"Artificial neural network\" refers to a biologically inspired sub-field of artificial \n",
            "intelligence  modeled  after  the  brain.\n",
            "\n",
            "Document 77: Similarity Score: 0.28889328241348267\n",
            "Document Text: Hardware dependence: \n",
            "\n",
            "Artificial  neural  networks  need  processors  with  parallel  processing  power,  as  per  their \n",
            "structure.\n",
            "\n",
            "Document 1: Similarity Score: 0.28814294934272766\n",
            "Document Text: An  Artificial  neural  network  is  usually  a \n",
            "computational network based on biological neural networks that construct the structure \n",
            "of the human brain.\n",
            "\n",
            "Document 61: Similarity Score: 0.2873212397098541\n",
            "Document Text: Activation functions  choose  whether a  node  should  fire or  not.\n",
            "\n",
            "Document 131: Similarity Score: 0.28705140948295593\n",
            "Document Text: Perceptron model is also treated as one of the best and simplest types of Artificial Neural \n",
            "networks.\n",
            "\n",
            "Document 48: Similarity Score: 0.2869442105293274\n",
            "Document Text: The output of ANNs is mostly dependent \n",
            "on these parameters.\n",
            "\n",
            "Document 109: Similarity Score: 0.285794734954834\n",
            "Document Text: ?x)) \n",
            "\n",
            "Where ?\n",
            "\n",
            "Document 50: Similarity Score: 0.2857034206390381\n",
            "Document Text: Each node in the ANN has some weight.\n",
            "\n",
            "Document 163: Similarity Score: 0.28556013107299805\n",
            "Document Text: The main objective of the single-layer perceptron model is to \n",
            "analyze the linearly separable objects with binary outcomes.\n",
            "\n",
            "Document 199: Similarity Score: 0.28529706597328186\n",
            "Document Text: 6.\n",
            "\n",
            "Document 29: Similarity Score: 0.283964604139328\n",
            "Document Text: A Computer Like a Brain \n",
            "\n",
            "Modern neuroscientists often discuss the brain as a type of computer.\n",
            "\n",
            "Document 164: Similarity Score: 0.283916711807251\n",
            "Document Text: In  a  single  layer  perceptron  model,  its  algorithms  do  not  contain  recorded  data,  so  it \n",
            "begins  with  inconstantly  allocated  input  for  weight  parameters.\n",
            "\n",
            "Document 81: Similarity Score: 0.28304389119148254\n",
            "Document Text: The presentation mechanism to be resolved here will \n",
            "directly impact the performance of the network.\n",
            "\n",
            "Document 240: Similarity Score: 0.2810814678668976\n",
            "Document Text: Here the target does not \n",
            "match with calculated output.\n",
            "\n",
            "Document 65: Similarity Score: 0.2806074023246765\n",
            "Document Text: Storing data on the entire network: \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\fData that is used in traditional programming is stored on the whole network, not on a \n",
            "database.\n",
            "\n",
            "Document 221: Similarity Score: 0.28041255474090576\n",
            "Document Text: Hence we need to update the weights.\n",
            "\n",
            "Document 178: Similarity Score: 0.28011006116867065\n",
            "Document Text: Further,  it  can  also  implement  logic  gates  such  as  AND,  OR,  XOR, \n",
            "NAND, NOT, XNOR, NOR.\n",
            "\n",
            "Document 72: Similarity Score: 0.27762284874916077\n",
            "Document Text: Disadvantages of Artificial Neural Network: \n",
            "\n",
            "Assurance of proper network structure: \n",
            "\n",
            "There is no particular guideline for determining the structure of artificial neural networks.\n",
            "\n",
            "Document 4: Similarity Score: 0.27746453881263733\n",
            "Document Text: What is Artificial Neural Network?\n",
            "\n",
            "Document 148: Similarity Score: 0.27702438831329346\n",
            "Document Text: How does Perceptron work?\n",
            "\n",
            "Document 234: Similarity Score: 0.2758842408657074\n",
            "Document Text: Here the target does not \n",
            "match with calculated output.\n",
            "\n",
            "Document 243: Similarity Score: 0.2757798135280609\n",
            "Document Text: For Training Instance 2: A=0, B=1 and Target = 1 \n",
            "\n",
            "wi.xi = 0*1.1 + 1*1.1 = 1.1 \n",
            "\n",
            "This is not greater than the threshold of 1, so the output = 0.\n",
            "\n",
            "Document 135: Similarity Score: 0.27540361881256104\n",
            "Document Text: In Machine Learning, binary classifiers are defined as the function that helps in deciding \n",
            "whether  input  data  can  be  represented  as  vectors  of  numbers  and  belongs  to  some \n",
            "specific class./  \n",
            "\n",
            "Binary  classifiers  can  be  considered  as  linear  classifiers.\n",
            "\n",
            "Document 262: Similarity Score: 0.2739865183830261\n",
            "Document Text: The direction of steepest can be found by computing the derivative of E with respect to \n",
            "each component of the vector w. This vector derivative is called the gradient of E with \n",
            "respect to w, written as, \n",
            "\n",
            "The gradient specifies the direction of steepest increase of E, the training rule for \n",
            "gradient descent is \n",
            "\n",
            "Here η is a positive constant called the learning rate, which determines the step size in \n",
            "the gradient descent search.\n",
            "\n",
            "Document 90: Similarity Score: 0.27381810545921326\n",
            "Document Text: The association between the neurons outputs and \n",
            "neuron  inputs  can  be  viewed  as  the  directed  edges  with  weights.\n",
            "\n",
            "Document 191: Similarity Score: 0.2726069390773773\n",
            "Document Text: 2.\n",
            "\n",
            "Document 33: Similarity Score: 0.2722589671611786\n",
            "Document Text: They perform many operations \n",
            "\n",
            "on a set of data, one at a time.\n",
            "\n",
            "Document 139: Similarity Score: 0.27173104882240295\n",
            "Document Text: Each input node contains a real numerical value.\n",
            "\n",
            "Document 194: Similarity Score: 0.2715756893157959\n",
            "Document Text: Initially, weights are multiplied with input features, and the decision is made whether the \n",
            "\n",
            "neuron is fired or not.\n",
            "\n",
            "Document 180: Similarity Score: 0.27094078063964844\n",
            "Document Text: o \n",
            "\n",
            "o \n",
            "\n",
            "o \n",
            "\n",
            "It works well with both small and large input data.\n",
            "\n",
            "Document 224: Similarity Score: 0.27083703875541687\n",
            "Document Text: For Training Instance 2: A=0, B=1 and Target = 0 \n",
            "\n",
            "wi.xi = 0*0.7 + 1*0.6 = 0.6 \n",
            "\n",
            "This is not greater than the threshold of 1, so the output = 0.\n",
            "\n",
            "Document 179: Similarity Score: 0.2700576186180115\n",
            "Document Text: Advantages of Multi-Layer Perceptron: \n",
            "\n",
            "o  A multi-layered perceptron model can be used to solve complex non-linear problems.\n",
            "\n",
            "Document 100: Similarity Score: 0.26986032724380493\n",
            "Document Text: The activation function refers to the set of transfer functions used to achieve the desired \n",
            "output.\n",
            "\n",
            "Document 85: Similarity Score: 0.26849374175071716\n",
            "Document Text: In the present time, we have investigated the pros of artificial neural \n",
            "\n",
            "networks  and  the  issues  encountered  in  the  course  of  their  utilization.\n",
            "\n",
            "Document 213: Similarity Score: 0.2682165801525116\n",
            "Document Text: One way to learn an acceptable weight vector is to begin with random weights, then \n",
            "iteratively apply the perceptron to each training example, modifying the perceptron \n",
            "weights whenever it misclassifies an example.\n",
            "\n",
            "Document 91: Similarity Score: 0.2675691843032837\n",
            "Document Text: The  Artificial  Neural \n",
            "Network receives the input signal from the external source in the form of a pattern and \n",
            "image  in  the  form  of  a  vector.\n",
            "\n",
            "Document 220: Similarity Score: 0.2674202620983124\n",
            "Document Text: Here the target does not \n",
            "match with the calculated output.\n",
            "\n",
            "Document 245: Similarity Score: 0.2647700607776642\n",
            "Document Text: For Training Instance 3: A=1, B=0 and Target = 1 \n",
            "\n",
            "wi.xi = 1*1.1 + 0*1.1 = 1.1 \n",
            "\n",
            "This is not greater than the threshold of 1, so the output = 0.\n",
            "\n",
            "Document 241: Similarity Score: 0.261618047952652\n",
            "Document Text: Now, \n",
            "\n",
            "Weights w1 = 1.1, w2 = 1.1, Threshold = 1 and Learning Rate n = 0.5 are given \n",
            "\n",
            "See also  Locally Weighted Regression Algorithm in Python \n",
            "\n",
            "For Training Instance 1: A=0, B=0 and Target = 0 \n",
            "\n",
            "wi.xi = 0*2.2 + 0*1.1 = 0 \n",
            "\n",
            "This is not greater than the threshold of 1, so the output = 0.\n",
            "\n",
            "Document 247: Similarity Score: 0.25977271795272827\n",
            "Document Text: For Training Instance 4: A=1, B=1 and Target = 1 \n",
            "\n",
            "wi.xi = 1*1.1 + 1*1.1 = 2.2 \n",
            "\n",
            " \n",
            " \n",
            " \n",
            "\fThis is greater than the threshold of 1, so the output = 1.\n",
            "\n",
            "Document 10: Similarity Score: 0.2594310939311981\n",
            "Document Text: Dendrites from Biological Neural Network represent inputs in Artificial Neural Networks, \n",
            "cell nucleus represents Nodes, synapse represents Weights, and Axon represents Output.\n",
            "\n",
            "Document 196: Similarity Score: 0.2579081356525421\n",
            "Document Text: The activation function applies a step rule to check whether the weight function is greater \n",
            "\n",
            "than zero.\n",
            "\n",
            "Document 146: Similarity Score: 0.2574305534362793\n",
            "Document Text: Types of Activation functions: \n",
            "\n",
            "o  Sign function \n",
            "\n",
            " \n",
            "\fo  Step function, and \n",
            "\n",
            "o  Sigmoid function \n",
            "\n",
            "The  data  scientist  uses  the  activation  function  to  take  a  subjective  decision  based  on \n",
            "various problem statements and forms the desired outputs.\n",
            "\n",
            "Document 53: Similarity Score: 0.25315818190574646\n",
            "Document Text: After the transfer function has calculated the sum, the activation function \n",
            "obtains the result.\n",
            "\n",
            "Document 108: Similarity Score: 0.2517014145851135\n",
            "Document Text: The function \n",
            "is defined as: \n",
            "\n",
            "F(x) = (1/1 + exp(-???\n",
            "\n",
            "Document 185: Similarity Score: 0.24970467388629913\n",
            "Document Text: o  The model functioning depends on the quality of the training.\n",
            "\n",
            "Document 165: Similarity Score: 0.24911019206047058\n",
            "Document Text: Further,  it  sums  up  all \n",
            "inputs (weight).\n",
            "\n",
            "Document 30: Similarity Score: 0.24897432327270508\n",
            "Document Text: Neural networks \n",
            "\n",
            "aim to do the opposite: build a computer that functions like a brain.\n",
            "\n",
            "Document 36: Similarity Score: 0.24813823401927948\n",
            "Document Text: It's highly interconnected.\n",
            "\n",
            "Document 18: Similarity Score: 0.2471068948507309\n",
            "Document Text: \"OR\" gate, which takes two \n",
            "inputs.\n",
            "\n",
            "Document 78: Similarity Score: 0.2463684380054474\n",
            "Document Text: Therefore, the realization of the equipment is dependent.\n",
            "\n",
            "Document 26: Similarity Score: 0.2462841421365738\n",
            "Document Text: After the transfer function has calculated the sum, the activation function \n",
            "obtains the result.\n",
            "\n",
            "Document 45: Similarity Score: 0.24584199488162994\n",
            "Document Text: It  performs  all  the \n",
            "calculations to find hidden features and patterns.\n",
            "\n",
            "Document 153: Similarity Score: 0.24462731182575226\n",
            "Document Text: This  step  function  or  Activation  function  plays  a  vital  role  in  ensuring  that  output  is \n",
            "mapped between required values (0,1) or (-1,1).\n",
            "\n",
            "Document 239: Similarity Score: 0.24223467707633972\n",
            "Document Text: For Training Instance 3: A=1, B=0 and Target = 1 \n",
            "\n",
            "wi.xi = 1*0.6 + 0*1.1 = 0.6 \n",
            "\n",
            "This is not greater than the threshold of 1, so the output = 0.\n",
            "\n",
            "Document 112: Similarity Score: 0.2414398342370987\n",
            "Document Text: Types of Artificial Neural Networks \n",
            "\n",
            "There are two important types of Artificial Neural Networks – \n",
            "\n",
            "  FeedForward Neural Network \n",
            "  FeedBack Neural Network \n",
            "\n",
            "FeedForward Artificial Neural Networks \n",
            "\n",
            "In the feedforward ANNs, the flow of information takes place only in one \n",
            "direction.\n",
            "\n",
            "Document 73: Similarity Score: 0.24112531542778015\n",
            "Document Text: The appropriate network structure is accomplished through experience, trial, and error.\n",
            "\n",
            "Document 142: Similarity Score: 0.24009199440479279\n",
            "Document Text: Weight  is  directly \n",
            "proportional  to  the  strength  of  the  associated  input  neuron  in  deciding  the  output.\n",
            "\n",
            "Document 138: Similarity Score: 0.23613499104976654\n",
            "Document Text: These are as follows: \n",
            "\n",
            "o \n",
            "\n",
            "Input Nodes or Input Layer: \n",
            "\n",
            "This is the primary component of Perceptron which accepts the initial data into the system \n",
            "for further processing.\n",
            "\n",
            "Document 64: Similarity Score: 0.23529379069805145\n",
            "Document Text: Advantages of Artificial Neural Network (ANN) \n",
            "\n",
            "Parallel processing capability: \n",
            "\n",
            "Artificial neural  networks  have a numerical value  that can perform more than one task \n",
            "simultaneously.\n",
            "\n",
            "Document 16: Similarity Score: 0.2342960387468338\n",
            "Document Text: We can say that the human brain is made up \n",
            "of incredibly amazing parallel processors.\n",
            "\n",
            "Document 9: Similarity Score: 0.23421809077262878\n",
            "Document Text: The typical Artificial Neural Network looks something like the given figure.\n",
            "\n",
            "Document 261: Similarity Score: 0.23357264697551727\n",
            "Document Text: How to calculate the direction of steepest descent along the error surface?\n",
            "\n",
            "Document 230: Similarity Score: 0.23309731483459473\n",
            "Document Text: Hence the final weights are w1= 0.7 and w2 = 0.6, Threshold = 1 and Learning Rate n = \n",
            "0.5.\n",
            "\n",
            "Document 156: Similarity Score: 0.23276440799236298\n",
            "Document Text: Perceptron model works in two important steps as follows: \n",
            "\n",
            "Step-1 \n",
            "\n",
            "In the first step first, multiply all input values with corresponding weight values and then \n",
            "add them to determine the weighted sum.\n",
            "\n",
            "Document 82: Similarity Score: 0.23250426352024078\n",
            "Document Text: It relies on the user's abilities.\n",
            "\n",
            "Document 258: Similarity Score: 0.23041225969791412\n",
            "Document Text: Derivation of Delta Rule \n",
            "\n",
            "The delta training rule is best understood by considering the task of training an \n",
            "unthresholded perceptron; that is, a linear unit for which the output o is given by \n",
            "\n",
            " \n",
            " \n",
            " \n",
            "\fThus, a linear unit corresponds to the first stage of a perceptron, without the threshold.\n",
            "\n",
            "Document 93: Similarity Score: 0.22985662519931793\n",
            "Document Text: Afterward, each of the input is multiplied by its corresponding weights ( these weights are \n",
            "the details utilized by the artificial neural networks to solve a specific problem ).\n",
            "\n",
            "Document 184: Similarity Score: 0.22972624003887177\n",
            "Document Text: In multi-layer Perceptron, it is difficult to predict how much the dependent variable affects \n",
            "\n",
            "each independent variable.\n",
            "\n",
            "Document 204: Similarity Score: 0.22864533960819244\n",
            "Document Text: Future of Perceptron \n",
            "\n",
            "The future of the Perceptron model is much bright and significant as it helps to interpret \n",
            "data by building intuitive patterns and applying them in the future.\n",
            "\n",
            "Document 141: Similarity Score: 0.22687290608882904\n",
            "Document Text: This  is \n",
            "another  most  important  parameter  of  Perceptron  components.\n",
            "\n",
            "Document 101: Similarity Score: 0.22538307309150696\n",
            "Document Text: There is a different kind of the activation function, but primarily either linear or \n",
            "non-linear sets of functions.\n",
            "\n",
            "Document 11: Similarity Score: 0.22431452572345734\n",
            "Document Text: Relationship between Biological neural network and artificial neural network: \n",
            "\n",
            "Biological Neural Network \n",
            "\n",
            "Artificial Neural Network \n",
            "\n",
            "Dendrites \n",
            "\n",
            "Cell nucleus \n",
            "\n",
            "Synapse \n",
            "\n",
            "Axon \n",
            "\n",
            "Inputs \n",
            "\n",
            "Nodes \n",
            "\n",
            "Weights \n",
            "\n",
            "Output \n",
            "\n",
            "An Artificial Neural Network in the field of Artificial intelligence where it attempts to \n",
            "mimic the network of neurons makes up a human brain so that computers will have an \n",
            "option to understand things and make decisions in a human-like manner.\n",
            "\n",
            "Document 158: Similarity Score: 0.2239668071269989\n",
            "Document Text: ∑wi*xi + b \n",
            "\n",
            "Step-2 \n",
            "\n",
            "In the second step, an activation function is applied with the above-mentioned weighted \n",
            "sum, which gives us output either in binary form or a continuous value as follows: \n",
            "\n",
            "Y = f(∑wi*xi + b) \n",
            "\n",
            " \n",
            "\fTypes of Perceptron Models \n",
            "\n",
            "Based on the layers, Perceptron models are divided into two types.\n",
            "\n",
            "Document 159: Similarity Score: 0.22379004955291748\n",
            "Document Text: These are as follows: \n",
            "\n",
            "1.\n",
            "\n",
            "Document 209: Similarity Score: 0.22258342802524567\n",
            "Document Text: More precisely, given inputs x1 through xn, the output o(x1, .\n",
            "\n",
            "Document 235: Similarity Score: 0.22100912034511566\n",
            "Document Text: Now, \n",
            "\n",
            "Weights w1 = 0.6, w2 = 1.1, Threshold = 1 and Learning Rate n = 0.5 are given \n",
            "\n",
            "For Training Instance 1: A=0, B=0 and Target = 0 \n",
            "\n",
            "wi.xi = 0*0.6 + 0*1.1 = 0 \n",
            "\n",
            "This is not greater than the threshold of 1, so the output = 0.\n",
            "\n",
            "Document 106: Similarity Score: 0.22008487582206726\n",
            "Document Text: Sigmoidal Hyperbolic: \n",
            "\n",
            "The Sigmoidal Hyperbola function is generally seen as an \"S\" shaped curve.\n",
            "\n",
            "Document 129: Similarity Score: 0.2198469638824463\n",
            "Document Text: Perceptron  is  Machine  Learning  algorithm  for  supervised  learning  of  various  binary \n",
            "classification tasks.\n",
            "\n",
            "Document 259: Similarity Score: 0.21941784024238586\n",
            "Document Text: In order to derive a weight learning rule for linear units, let us begin by specifying a \n",
            "measure for the training error of a hypothesis (weight vector), relative to the training \n",
            "examples.\n",
            "\n",
            "Document 136: Similarity Score: 0.214814230799675\n",
            "Document Text: In  simple  words,  we  can \n",
            "understand it as a classification algorithm that can predict linear predictor function \n",
            "in terms of weight and feature vectors.\n",
            "\n",
            "Document 22: Similarity Score: 0.2143884152173996\n",
            "Document Text: Our brain \n",
            "does  not  perform  the  same  task.\n",
            "\n",
            "Document 46: Similarity Score: 0.21425208449363708\n",
            "Document Text: Output Layer: \n",
            "\n",
            "The input goes through a series of transformations using the hidden layer, which finally \n",
            "results in output that is conveyed using this layer.\n",
            "\n",
            "Document 94: Similarity Score: 0.21405905485153198\n",
            "Document Text: In general \n",
            "terms,  these  weights  normally  represent  the  strength  of  the  interconnection  between \n",
            "neurons inside the artificial neural network.\n",
            "\n",
            "Document 47: Similarity Score: 0.2134343832731247\n",
            "Document Text: In a neural network, there are multiple parameters and hyperparameters that \n",
            "affect the performance of the model.\n",
            "\n",
            "Document 176: Similarity Score: 0.21330782771110535\n",
            "Document Text: Instead of linear, activation function can be executed \n",
            "as sigmoid, TanH, ReLU, etc., for deployment.\n",
            "\n",
            "Document 27: Similarity Score: 0.21236552298069\n",
            "Document Text: Based on the output received, the activation functions fire \n",
            "the appropriate result from the node.\n",
            "\n",
            "Document 168: Similarity Score: 0.2122012823820114\n",
            "Document Text: However,  this \n",
            "model consists of a few discrepancies triggered when multiple weight inputs values are \n",
            "fed  into  the  model.\n",
            "\n",
            "Document 226: Similarity Score: 0.21070221066474915\n",
            "Document Text: For Training Instance 3: A=1, B=0 and Target = 0 \n",
            "\n",
            "wi.xi = 1*0.7 + 0*0.6 = 0.7 \n",
            "\n",
            "This is not greater than the threshold of 1, so the output = 0.\n",
            "\n",
            "Document 249: Similarity Score: 0.20988187193870544\n",
            "Document Text: Final wieghts w1 = 1.1, w2 = 1.1 Threshold = 1 and Learning Rate n = 0.5.\n",
            "\n",
            "Document 203: Similarity Score: 0.20955774188041687\n",
            "Document Text: If input \n",
            "\n",
            "vectors are non-linear, it is not easy to classify them properly.\n",
            "\n",
            "Document 195: Similarity Score: 0.20643731951713562\n",
            "Document Text: 4.\n",
            "\n",
            "Document 68: Similarity Score: 0.20603634417057037\n",
            "Document Text: The \n",
            "loss of performance here relies upon the significance of missing data.\n",
            "\n",
            "Document 125: Similarity Score: 0.20578788220882416\n",
            "Document Text: This algorithm enables neurons to learn elements and processes them one by \n",
            "one during preparation.\n",
            "\n",
            "Document 86: Similarity Score: 0.2045644372701645\n",
            "Document Text: It  should  not  be \n",
            "\n",
            "overlooked  that  the  cons  of  ANN  networks,  which  are  a  flourishing  science  branch,  are \n",
            "\n",
            "eliminated individually, and their pros are increasing day by day.\n",
            "\n",
            "Document 21: Similarity Score: 0.20353145897388458\n",
            "Document Text: Here the output depends upon input.\n",
            "\n",
            "Document 8: Similarity Score: 0.20344896614551544\n",
            "Document Text: Play Video \n",
            "\n",
            "The given figure illustrates the typical diagram of Biological Neural Network.\n",
            "\n",
            "Document 219: Similarity Score: 0.2023017555475235\n",
            "Document Text: For Training Instance 2: A=1, B=0 and Target = 0 \n",
            "\n",
            "wi.xi = 1*1.2 + 0*0.6 = 1.2 \n",
            "\n",
            "This is greater than the threshold of 1, so the output = 1.\n",
            "\n",
            "Document 80: Similarity Score: 0.19977541267871857\n",
            "Document Text: Problems must be converted into numerical values \n",
            "before being introduced to ANN.\n",
            "\n",
            "Document 161: Similarity Score: 0.1963580846786499\n",
            "Document Text: Multi-layer Perceptron model \n",
            "\n",
            "Single Layer Perceptron Model: \n",
            "\n",
            "This  is  one  of  the  easiest  Artificial  neural  networks  (ANN)  types.\n",
            "\n",
            "Document 198: Similarity Score: 0.19519151747226715\n",
            "Document Text: The linear decision boundary is drawn, enabling the distinction between the two linearly \n",
            "\n",
            "separable classes +1 and -1.\n",
            "\n",
            "Document 250: Similarity Score: 0.19449275732040405\n",
            "Document Text: Gradient Descent And Delta Rule \n",
            "\n",
            "Gradient Descent and Delta Rule \n",
            "\n",
            "A set of data points are said to be linearly separable if the data can be divided into two \n",
            "classes using a straight line.\n",
            "\n",
            "Document 31: Similarity Score: 0.1944730132818222\n",
            "Document Text: Of course, we only have a cursory understanding of the brain's extremely complex \n",
            "\n",
            "functions, but by creating a simplified simulation of how the brain processes data, we \n",
            "\n",
            "can build a type of computer that functions very differently from a standard one.\n",
            "\n",
            "Document 54: Similarity Score: 0.1942172646522522\n",
            "Document Text: Based on the output received, the activation functions fire \n",
            "the appropriate result from the node.\n",
            "\n",
            "Document 193: Similarity Score: 0.19269712269306183\n",
            "Document Text: 3.\n",
            "\n",
            "Document 217: Similarity Score: 0.1922178417444229\n",
            "Document Text: For Training Instance 2: A=0, B=1 and Target = 0 \n",
            "\n",
            "See also  Linear Regression Solved Example with One Independent Variable \n",
            "\n",
            "wi.xi = 0*1.2 + 1*0.6 = 0.6 \n",
            "\n",
            "This is not greater than the threshold of 1, so the output = 0.\n",
            "\n",
            "Document 75: Similarity Score: 0.19153667986392975\n",
            "Document Text: When ANN produces a testing solution, it does not \n",
            "provide insight concerning why and how.\n",
            "\n",
            "Document 87: Similarity Score: 0.19076700508594513\n",
            "Document Text: It means that artificial neural \n",
            "\n",
            "networks will turn into an irreplaceable part of our lives progressively important.\n",
            "\n",
            "Document 228: Similarity Score: 0.18930043280124664\n",
            "Document Text: For Training Instance 4: A=1, B=1 and Target = 1 \n",
            "\n",
            "wi.xi = 1*0.7 + 1*0.6 = 1.3 \n",
            "\n",
            "This is greater than the threshold of 1, so the output = 1.\n",
            "\n",
            "Document 63: Similarity Score: 0.18853817880153656\n",
            "Document Text: There  are  distinctive  activation  functions \n",
            "available that can be applied upon the sort of task we are performing.\n",
            "\n",
            "Document 189: Similarity Score: 0.18592824041843414\n",
            "Document Text: 1.\n",
            "\n",
            "Document 89: Similarity Score: 0.18565239012241364\n",
            "Document Text: Artificial Neural Network can be best represented as a weighted directed graph, where \n",
            "the artificial neurons form the nodes.\n",
            "\n",
            "Document 256: Similarity Score: 0.1839704066514969\n",
            "Document Text: This rule is important because gradient descent provides the basis for the \n",
            "BACKPROPAGATON algorithm, which can learn networks with many interconnected \n",
            "units.\n",
            "\n",
            "Document 62: Similarity Score: 0.18345628678798676\n",
            "Document Text: Only those \n",
            "who  are  fired  make  it  to  the  output  layer.\n",
            "\n",
            "Document 104: Similarity Score: 0.1831032782793045\n",
            "Document Text: Here, to accomplish this, \n",
            "there is a threshold value set up.\n",
            "\n",
            "Document 57: Similarity Score: 0.18213149905204773\n",
            "Document Text: Based on the value that the node has fired, we obtain the final output.\n",
            "\n",
            "Document 174: Similarity Score: 0.18093308806419373\n",
            "Document Text: In  this  stage,  the  error  between  actual  output  and  demanded \n",
            "\n",
            "originated backward on the output layer and ended on the input layer.\n",
            "\n",
            "Document 127: Similarity Score: 0.17987273633480072\n",
            "Document Text: Let's start with the basic \n",
            "introduction of Perceptron.\n",
            "\n",
            "Document 122: Similarity Score: 0.17950426042079926\n",
            "Document Text: Perceptron is a building block of an Artificial Neural Network.\n",
            "\n",
            "Document 231: Similarity Score: 0.176737979054451\n",
            "Document Text: OR GATE Perceptron Training Rule Machine \n",
            "Learning \n",
            "\n",
            "Truth Table of OR Logical GATE is, \n",
            "\n",
            "Weights w1 = 0.6, w2 = 0.6, Threshold = 1 and Learning Rate n = 0.5 are given \n",
            "\n",
            "For Training Instance 1: A=0, B=0 and Target = 0 \n",
            "\n",
            "wi.xi = 0*0.6 + 0*0.6 = 0 \n",
            "\n",
            "This is not greater than the threshold of 1, so the output = 0.\n",
            "\n",
            "Document 260: Similarity Score: 0.17606571316719055\n",
            "Document Text: Although there are many ways to define this error, one common measure is \n",
            "\n",
            "where D is the set of training examples, ‘td’ is the target output for training example ‘d’, \n",
            "and od is the output of the linear unit for training example ‘d’.\n",
            "\n",
            "Document 253: Similarity Score: 0.17583294212818146\n",
            "Document Text: A second training rule, called the delta rule, is designed to overcome this difficulty.\n",
            "\n",
            "Document 15: Similarity Score: 0.17547264695167542\n",
            "Document Text: In the human brain, data is stored in \n",
            "such a manner as to be distributed, and we can extract more than one piece of this data \n",
            "when necessary from our memory parallelly.\n",
            "\n",
            "Document 67: Similarity Score: 0.17143021523952484\n",
            "Document Text: Capability to work with incomplete knowledge: \n",
            "\n",
            "After ANN training, the information may produce output even with inadequate data.\n",
            "\n",
            "Document 181: Similarity Score: 0.17008303105831146\n",
            "Document Text: It helps us to obtain quick predictions after the training.\n",
            "\n",
            "Document 202: Similarity Score: 0.16671255230903625\n",
            "Document Text: o  Perceptron can only be used to classify the linearly separable sets of input vectors.\n",
            "\n",
            "Document 25: Similarity Score: 0.16584251821041107\n",
            "Document Text: A transfer function \n",
            "is used for calculating the weighted sum of the inputs and the bias.\n",
            "\n",
            "Document 52: Similarity Score: 0.1658419817686081\n",
            "Document Text: A transfer function \n",
            "is used for calculating the weighted sum of the inputs and the bias.\n",
            "\n",
            "Document 255: Similarity Score: 0.16516533493995667\n",
            "Document Text: The key idea behind the delta rule is to use gradient descent to search the hypothesis \n",
            "space of possible weight vectors to find the weights that best fit the training examples.\n",
            "\n",
            "Document 254: Similarity Score: 0.16481971740722656\n",
            "Document Text: If the training examples are not linearly separable, the delta rule converges toward a \n",
            "best-fit approximation to the target concept.\n",
            "\n",
            "Document 40: Similarity Score: 0.16406114399433136\n",
            "Document Text: The architecture of an artificial neural network: \n",
            "\n",
            "To understand the concept of the architecture of an artificial neural network, we have to \n",
            "understand what a neural network consists of.\n",
            "\n",
            "Document 117: Similarity Score: 0.1636500060558319\n",
            "Document Text: Feedback Artificial Neural Networks \n",
            "\n",
            "In the feedback ANNs, the feedback loops are a part of it.\n",
            "\n",
            "Document 152: Similarity Score: 0.16195577383041382\n",
            "Document Text: This activation function is also known as the step function and is represented \n",
            "by 'f'.\n",
            "\n",
            "Document 251: Similarity Score: 0.16126692295074463\n",
            "Document Text: If the data is not divided into two classes using a straight \n",
            "line, such data points are said to be called non-linearly separable data.\n",
            "\n",
            "Document 216: Similarity Score: 0.15991567075252533\n",
            "Document Text: Perceptron Training Rule \n",
            "\n",
            "Perceptron_training_rule (X, η) \n",
            "\n",
            " initialize w (wi <- an initial (small) random value) \n",
            "\n",
            " repeat \n",
            "\n",
            "     for each training instance (x, tx) ∈ X \n",
            "\n",
            "         compute the real output ox = Activation(Summation(w.x)) \n",
            "\n",
            "         if (tx ≠ ox) \n",
            "\n",
            " \n",
            " \n",
            "\f             for each wi \n",
            "\n",
            "                 wi <- wi + ∆𝑤𝑖 \n",
            "\n",
            "                 ∆𝑤𝑖 <- η (tx - ox)xi \n",
            "\n",
            "             end for \n",
            "\n",
            "         end if \n",
            "\n",
            "     end for \n",
            "\n",
            " until all the training instances in X are correctly classified \n",
            "\n",
            " return w \n",
            "\n",
            "AND GATE Perceptron Training Rule \n",
            "Machine Learning \n",
            "\n",
            "ruth Table of AND Logical GATE is, \n",
            "\n",
            "Weights w1 = 1.2, w2 = 0.6, Threshold = 1 and Learning Rate n = 0.5 are given \n",
            "\n",
            "For Training Instance 1: A=0, B=0 and Target = 0 \n",
            "\n",
            "wi.xi = 0*1.2 + 0*0.6 = 0 \n",
            "\n",
            " \n",
            "\fThis is not greater than the threshold of 1, so the output = 0, Here the target is same as \n",
            "calculated output.\n",
            "\n",
            "Document 99: Similarity Score: 0.15829510986804962\n",
            "Document Text: Here, to keep the response in the limits of the desired value, a certain maximum value is \n",
            "benchmarked, and the total of weighted inputs is passed through the activation function.\n",
            "\n",
            "Document 167: Similarity Score: 0.15814733505249023\n",
            "Document Text: If the outcome is same as pre-determined or threshold value, then the performance of \n",
            "this  model  is  stated  as  satisfied,  and  weight  demand  does  not  change.\n",
            "\n",
            "Document 111: Similarity Score: 0.15624918043613434\n",
            "Document Text: is considered the Steepness parameter.\n",
            "\n",
            "Document 74: Similarity Score: 0.15431924164295197\n",
            "Document Text: Unrecognized behavior of the network: \n",
            "\n",
            "It is the most significant issue of ANN.\n",
            "\n",
            "Document 124: Similarity Score: 0.15371757745742798\n",
            "Document Text: Perceptron \n",
            "is  a  linear  Machine  Learning  algorithm  used  for  supervised  learning  for  various  binary \n",
            "classifiers.\n",
            "\n",
            "Document 192: Similarity Score: 0.15234097838401794\n",
            "Document Text: In Perceptron, the weight coefficient is automatically learned.\n",
            "\n",
            "Document 97: Similarity Score: 0.15029999613761902\n",
            "Document Text: Bias has the same input, and weight \n",
            "equals to 1.\n",
            "\n",
            "Document 102: Similarity Score: 0.14988961815834045\n",
            "Document Text: Some of the commonly used sets of activation functions are \n",
            "the Binary, linear, and Tan hyperbolic sigmoidal activation functions.\n",
            "\n",
            "Document 149: Similarity Score: 0.14931754767894745\n",
            "Document Text: In  Machine  Learning,  Perceptron  is  considered  as  a  single-layer  neural  network  that \n",
            "consists of four main parameters named input values (Input nodes), weights and Bias, net \n",
            "sum, and an activation function.\n",
            "\n",
            "Document 133: Similarity Score: 0.1485878974199295\n",
            "Document Text: Hence, we \n",
            "can  consider  it  as  a  single-layer  neural  network  with  four  main  parameters,  i.e.,  input \n",
            "values, weights and Bias, net sum, and an activation function.\n",
            "\n",
            "Document 157: Similarity Score: 0.14791667461395264\n",
            "Document Text: Mathematically, we can calculate the weighted \n",
            "sum as follows: \n",
            "\n",
            "∑wi*xi = x1*w1 + x2*w2 +…wn*xn \n",
            "\n",
            "Add  a  special  term  called  bias  'b'  to  this  weighted  sum  to  improve  the  model's \n",
            "performance.\n",
            "\n",
            "Document 166: Similarity Score: 0.14668412506580353\n",
            "Document Text: After adding all inputs, if the total sum of all inputs is more than a pre-\n",
            "determined value, the model gets activated and shows the output value as +1.\n",
            "\n",
            "Document 121: Similarity Score: 0.14520889520645142\n",
            "Document Text: It  is  the  primary  step  to  learn  Machine  Learning  and  Deep  Learning \n",
            "technologies,  which  consists  of  a  set  of  weights,  input  values  or  scores,  and  a \n",
            "threshold.\n",
            "\n",
            "Document 113: Similarity Score: 0.1446271687746048\n",
            "Document Text: That is, the flow of information is from the input layer to the hidden \n",
            "layer and finally to the output.\n",
            "\n",
            "Document 60: Similarity Score: 0.14274050295352936\n",
            "Document Text: It determines weighted total is passed as an input to an activation function to produce \n",
            "the  output.\n",
            "\n",
            "Document 17: Similarity Score: 0.13893403112888336\n",
            "Document Text: We can understand the artificial neural network with an example, consider an example of \n",
            "a digital logic gate that takes an input and gives an output.\n",
            "\n",
            "Document 28: Similarity Score: 0.13842567801475525\n",
            "Document Text: For example, if the output received is \n",
            "above 0.5, the activation function fires a 1 otherwise it remains 0.\n",
            "\n",
            "Document 96: Similarity Score: 0.13676856458187103\n",
            "Document Text: If the weighted sum is equal to zero, then bias is added to make the output non-zero or \n",
            "something else to scale up to the system's response.\n",
            "\n",
            "Document 55: Similarity Score: 0.13288597762584686\n",
            "Document Text: For example, if the output received is \n",
            "above 0.5, the activation function fires a 1 otherwise it remains 0.\n",
            "\n",
            "Document 105: Similarity Score: 0.1300082951784134\n",
            "Document Text: If the net weighted input of neurons is more than 1, then \n",
            "the final output of the activation function is returned as one or else the output is returned \n",
            "as 0.\n",
            "\n",
            "Document 107: Similarity Score: 0.12534020841121674\n",
            "Document Text: Here the tan \n",
            "hyperbolic function is used to approximate output from the actual net input.\n",
            "\n",
            "Document 182: Similarity Score: 0.12312602996826172\n",
            "Document Text: It helps to obtain the same accuracy ratio with large as well as small data.\n",
            "\n",
            "Document 155: Similarity Score: 0.11920960247516632\n",
            "Document Text: Similarly, an input's bias value gives the ability \n",
            "to shift the activation function curve up or down.\n",
            "\n",
            "Document 186: Similarity Score: 0.11817280203104019\n",
            "Document Text: Perceptron Function \n",
            "\n",
            "Perceptron function ''f(x)'' can be achieved as output by multiplying the input 'x' with the \n",
            "learned weight coefficient 'w'.\n",
            "\n",
            "Document 58: Similarity Score: 0.1107005774974823\n",
            "Document Text: Then, \n",
            "using the error functions, we calculate the discrepancies between the \n",
            "predicted output and resulting output and adjust the weights of the neural \n",
            "network through a process known as backpropagation \n",
            "\n",
            "The artificial neural network takes input and computes the weighted sum of the inputs \n",
            "and includes a bias.\n",
            "\n",
            "Document 200: Similarity Score: 0.11048522591590881\n",
            "Document Text: If  the  added  sum  of  all  input  values  is  more  than  the  threshold  value,  it  must  have  an \n",
            "\n",
            "output signal; otherwise, no output will be shown.\n",
            "\n",
            "Document 79: Similarity Score: 0.10695784538984299\n",
            "Document Text: Difficulty of showing the issue to the network: \n",
            "\n",
            "\fANNs can work with numerical data.\n",
            "\n",
            "Document 83: Similarity Score: 0.10346371680498123\n",
            "Document Text: The duration of the network is unknown: \n",
            "\n",
            "The network is reduced to a specific value of the error, and this value does not give us \n",
            "optimum results.\n",
            "\n",
            "Document 71: Similarity Score: 0.1005508080124855\n",
            "Document Text: Having fault tolerance: \n",
            "\n",
            "Extortion of one or more cells of ANN does not prohibit it from generating output, and \n",
            "this feature makes the network fault-tolerance.\n",
            "\n",
            "Document 103: Similarity Score: 0.09937047958374023\n",
            "Document Text: Let us take a look at \n",
            "each of them in details: \n",
            "\n",
            "Binary: \n",
            "\n",
            " \n",
            "\fIn binary activation function, the output is either a one or a 0.\n",
            "\n",
            "Document 187: Similarity Score: 0.09714281558990479\n",
            "Document Text: Mathematically, we can express it as follows: \n",
            "\n",
            "f(x)=1; if w.x+b>0 \n",
            "\n",
            "otherwise, f(x)=0 \n",
            "\n",
            "\fo \n",
            "\n",
            "o \n",
            "\n",
            "o \n",
            "\n",
            "'w' represents real-valued weights vector \n",
            "\n",
            "'b' represents the bias \n",
            "\n",
            "'x' represents a vector of input x values.\n",
            "\n",
            "Document 154: Similarity Score: 0.09226492047309875\n",
            "Document Text: It is important to note that the weight of \n",
            "input is indicative of the strength of a node.\n",
            "\n",
            "Document 222: Similarity Score: 0.08872237801551819\n",
            "Document Text: Now, \n",
            "\n",
            "After updating weights are w1 = 0.7, w2 = 0.6 Threshold = 1 and Learning Rate n = 0.5 \n",
            "\n",
            "w1 = 0.7, w2 = 0.6 Threshold = 1 and Learning Rate n = 0.5 \n",
            "\n",
            "For Training Instance 1: A=0, B=0 and Target = 0 \n",
            "\n",
            " \n",
            " \n",
            " \n",
            "\fwi.xi = 0*0.7 + 0*0.6 = 0 \n",
            "\n",
            "This is not greater than the threshold of 1, so the output = 0.\n",
            "\n",
            "Document 173: Similarity Score: 0.08544940501451492\n",
            "Document Text: o  Backward Stage: In the backward stage, weight and bias values are modified as per the \n",
            "\n",
            "model's  requirement.\n",
            "\n",
            "Document 140: Similarity Score: 0.07108843326568604\n",
            "Document Text: o  Wight and Bias: \n",
            "\n",
            "Weight  parameter  represents  the  strength  of  the  connection  between  units.\n",
            "\n",
            "Document 151: Similarity Score: 0.05522942915558815\n",
            "Document Text: Then this weighted sum is applied to the activation function 'f' to obtain the desired \n",
            "output.\n",
            "\n",
            "Document 69: Similarity Score: 0.05110829323530197\n",
            "Document Text: Having a memory distribution: \n",
            "\n",
            "For  ANN  is  to  be  able  to  adapt,  it  is  important  to  determine  the  examples  and  to \n",
            "encourage the network according to the desired output by demonstrating these examples \n",
            "to  the  network.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cyg3wIV1o-lS"
      },
      "execution_count": 65,
      "outputs": []
    }
  ]
}